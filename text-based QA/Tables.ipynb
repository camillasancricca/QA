{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05de4b70",
   "metadata": {},
   "source": [
    "TABLES (1- MEDIAN; 2- THRESHOLD; 3- PRESENCE PERCENTAGE; 4- NON-ZERO MEDIAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179215a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# --- CONFIGURAZIONE GLOBALE FILES ---\n",
    "FILES_CONFIG = {\n",
    "    \"GR\": \"gr_gpt_ALL.csv\",\n",
    "    \"SYNT\": \"synt_gpt_ALL.csv\",\n",
    "    \"READ\": \"read_gpt_ALL.csv\",\n",
    "    \"INFDENS\": \"infdens_gpt_ALL.csv\",\n",
    "}\n",
    "\n",
    "# Funzione di caricamento comune\n",
    "def load_and_preprocess(path: str) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\" File not found: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "        if \"sentence_percentage\" in df.columns:\n",
    "            df.rename(columns={\"sentence_percentage\": \"percentage\"}, inplace=True)\n",
    "        if \"percentage\" not in df.columns:\n",
    "            if \"setting\" in df.columns:\n",
    "                mapping = {\"2s_3e\": 50, \"4s_3e\": 100}\n",
    "                df[\"percentage\"] = df[\"setting\"].map(mapping)\n",
    "            else:\n",
    "                raise ValueError(\"Missing 'percentage' or 'setting' column.\")\n",
    "        if \"manipulation\" not in df.columns:\n",
    "            raise ValueError(\"Missing 'manipulation' column\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c431256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYSIS 1 ---\n",
    "METRICS_ANALYSIS_1 = [\n",
    "    \"acc_median_final\",\n",
    "    \"comp_final_median\",\n",
    "    \"key_concept_coverage\",\n",
    "    \"Conciseness\",\n",
    "]\n",
    "\n",
    "# Cartella di output per tenere tutto ordinato\n",
    "OUTPUT_DIR_1 = \"results_medians\"\n",
    "OUTPUT_CSV_LONG = os.path.join(OUTPUT_DIR_1, \"final_table_long.csv\")\n",
    "OUTPUT_TXT_LONG = os.path.join(OUTPUT_DIR_1, \"final_table_long.txt\")\n",
    "\n",
    "def process_dataset_median(df: pd.DataFrame, dataset_name: str, metrics: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Calculates medians and pivots the table for a specific dataset.\"\"\"\n",
    "    grouped = df.groupby([\"manipulation\", \"percentage\"])[metrics].median().reset_index()\n",
    "\n",
    "    rows = []\n",
    "    for m in metrics:\n",
    "        temp = grouped.pivot(index=\"percentage\", columns=\"manipulation\", values=m)\n",
    "        temp.index = [f\"{m}_{int(idx)}\" for idx in temp.index]\n",
    "        rows.append(temp)\n",
    "\n",
    "    table = pd.concat(rows)\n",
    "    \n",
    "    table.columns = [f\"{dataset_name}_{col}\" for col in table.columns]\n",
    "    \n",
    "    return table\n",
    "\n",
    "print(\"Starting Standard Median Analysis...\")\n",
    "os.makedirs(OUTPUT_DIR_1, exist_ok=True)\n",
    "processed_tables_1 = []\n",
    "\n",
    "for name, path in FILES_CONFIG.items():\n",
    "    df = load_and_preprocess(path)\n",
    "    if df is not None:\n",
    "        try:\n",
    "            table = process_dataset_median(df, name, METRICS_ANALYSIS_1)\n",
    "            \n",
    "            # --- SALVE FOR EACH DIMENSION ---\n",
    "            single_csv_path = os.path.join(OUTPUT_DIR_1, f\"{name}_median.csv\")\n",
    "            table.to_csv(single_csv_path, encoding=\"utf-8\")\n",
    "            print(f\" -> Saved SINGLE table: {single_csv_path}\")\n",
    "            \n",
    "            processed_tables_1.append(table)\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {name}: {e}\")\n",
    "\n",
    "if processed_tables_1:\n",
    "    # UNIFIED TABLE\n",
    "    final_table_1 = pd.concat(processed_tables_1, axis=1).fillna(0)\n",
    "\n",
    "    print(\"\\n=== FINAL UNIFIED TABLE (MEDIANS) ===\")\n",
    "    table_str_1 = tabulate(final_table_1, headers=\"keys\", tablefmt=\"grid\", showindex=True)\n",
    "    print(table_str_1)\n",
    "\n",
    "    final_table_1.to_csv(OUTPUT_CSV_LONG, encoding=\"utf-8\")\n",
    "    with open(OUTPUT_TXT_LONG, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(table_str_1)\n",
    "    \n",
    "    print(f\"\\n Results saved to directory: {OUTPUT_DIR_1}\")\n",
    "else:\n",
    "    print(\"No data processed for Analysis 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b768e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYSIS 2 ---\n",
    "OUTPUT_DIR_2 = \"results_thresholds\"\n",
    "\n",
    "THRESHOLDS_06 = {\n",
    "    \"acc_median_final\": 0.6,\n",
    "    \"comp_final_median\": 0.6,\n",
    "    \"key_concept_coverage\": 0.6,\n",
    "    \"Conciseness\": {\"low\": 0.5, \"high\": 1.5},\n",
    "    \"Num_addition\": 2,\n",
    "    \"Num_direct_modifications\": 2,\n",
    "}\n",
    "\n",
    "THRESHOLDS_04 = {\n",
    "    \"acc_median_final\": 0.4,\n",
    "    \"comp_final_median\": 0.4,\n",
    "    \"key_concept_coverage\": 0.4,\n",
    "    \"Num_addition\": 4,\n",
    "    \"Num_direct_modifications\": 4,\n",
    "}\n",
    "\n",
    "THRESHOLD_SETS = {\"thr06\": THRESHOLDS_06, \"thr04\": THRESHOLDS_04}\n",
    "\n",
    "def calculate_pass_rate(df_grouped, metric: str, threshold: Any) -> List[pd.Series]:\n",
    "    series_list = []\n",
    "    if isinstance(threshold, dict) and \"low\" in threshold and \"high\" in threshold:\n",
    "        low, high = threshold[\"low\"], threshold[\"high\"]\n",
    "        s_below = df_grouped[metric].apply(lambda x: (x < low).sum() / x.count() * 100 if x.count() > 0 else 0.0)\n",
    "        s_below.name = f\"{metric}_below_{low}\"\n",
    "        series_list.append(s_below)\n",
    "        s_above = df_grouped[metric].apply(lambda x: (x > high).sum() / x.count() * 100 if x.count() > 0 else 0.0)\n",
    "        s_above.name = f\"{metric}_above_{high}\"\n",
    "        series_list.append(s_above)\n",
    "    elif metric in [\"Num_addition\", \"Num_direct_modifications\"]:\n",
    "        s_above = df_grouped[metric].apply(lambda x: (x > threshold).sum() / x.count() * 100 if x.count() > 0 else 0.0)\n",
    "        s_above.name = f\"{metric}_above_{threshold}\"\n",
    "        series_list.append(s_above)\n",
    "    else:\n",
    "        s_below = df_grouped[metric].apply(lambda x: (x < threshold).sum() / x.count() * 100 if x.count() > 0 else 0.0)\n",
    "        s_below.name = f\"{metric}_below_{threshold}\"\n",
    "        series_list.append(s_below)\n",
    "    return series_list\n",
    "\n",
    "def process_dataset_thresholds(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "    unique_pcts = sorted(df[\"percentage\"].dropna().unique())\n",
    "    manip_vals = sorted(df[\"manipulation\"].dropna().unique())\n",
    "    rows = []\n",
    "    for thr_set_name, thresholds in THRESHOLD_SETS.items():\n",
    "        for metric, thr_val in thresholds.items():\n",
    "            if metric not in df.columns: continue\n",
    "            for pct in unique_pcts:\n",
    "                df_pct = df[df[\"percentage\"] == pct]\n",
    "                grouped = df_pct.groupby(\"manipulation\")\n",
    "                results = calculate_pass_rate(grouped, metric, thr_val)\n",
    "                for res in results:\n",
    "                    res = res.reindex(manip_vals, fill_value=0.0)\n",
    "                    res.name = f\"{res.name}_{int(pct)}\"\n",
    "                    rows.append(res)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    table = pd.concat(rows, axis=1).T\n",
    "    table.columns = [f\"{dataset_name}_{col}\" for col in table.columns]\n",
    "    return table.fillna(0.0).round(2)\n",
    "\n",
    "print(\"Starting Threshold Analysis...\")\n",
    "os.makedirs(OUTPUT_DIR_2, exist_ok=True)\n",
    "processed_tables_2 = []\n",
    "\n",
    "for name, path in FILES_CONFIG.items():\n",
    "    df = load_and_preprocess(path)\n",
    "    if df is not None:\n",
    "        table = process_dataset_thresholds(df, name)\n",
    "        if not table.empty:\n",
    "            csv_path = os.path.join(OUTPUT_DIR_2, f\"{name}_thresholds.csv\")\n",
    "            table.to_csv(csv_path, encoding=\"utf-8\")\n",
    "            print(f\" -> Saved SINGLE table: {csv_path}\")\n",
    "            processed_tables_2.append(table)\n",
    "\n",
    "if processed_tables_2:\n",
    "    final_table_2 = pd.concat(processed_tables_2, axis=1).fillna(0.0).round(2)\n",
    "    final_csv_2 = os.path.join(OUTPUT_DIR_2, \"final_thresholds_unified.csv\")\n",
    "    final_table_2.to_csv(final_csv_2, encoding=\"utf-8\")\n",
    "    print(f\"\\n Final unified thresholds table saved to: {final_csv_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b12d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYSIS 3 ---\n",
    "OUTPUT_DIR_3 = \"results_presence\"\n",
    "METRICS_MAP_3 = {\"%DirectModifications\": \"Num_direct_modifications\", \"%Additions\": \"Num_addition\"}\n",
    "\n",
    "def calculate_presence(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "    unique_pcts = sorted(df[\"percentage\"].dropna().unique())\n",
    "    manip_vals = sorted(df[\"manipulation\"].dropna().unique())\n",
    "    rows = []\n",
    "    for display_name, col_name in METRICS_MAP_3.items():\n",
    "        if col_name not in df.columns: continue\n",
    "        for pct in unique_pcts:\n",
    "            s = df[df[\"percentage\"] == pct].groupby(\"manipulation\")[col_name].apply(\n",
    "                lambda x: (x > 0).sum() / x.count() * 100 if x.count() > 0 else 0.0\n",
    "            )\n",
    "            s = s.reindex(manip_vals, fill_value=0.0)\n",
    "            s.name = f\"{display_name}_{int(pct)}\"\n",
    "            rows.append(s)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    table = pd.concat(rows, axis=1).T\n",
    "    table.columns = [f\"{dataset_name}_{col}\" for col in table.columns]\n",
    "    return table.fillna(0.0).round(2)\n",
    "\n",
    "print(\"Starting Presence Analysis (Value > 0)...\")\n",
    "os.makedirs(OUTPUT_DIR_3, exist_ok=True)\n",
    "processed_tables_3 = []\n",
    "\n",
    "for name, path in FILES_CONFIG.items():\n",
    "    df = load_and_preprocess(path)\n",
    "    if df is not None:\n",
    "        try:\n",
    "            table = calculate_presence(df, name)\n",
    "            csv_path = os.path.join(OUTPUT_DIR_3, f\"{name}_presence.csv\")\n",
    "            table.to_csv(csv_path, encoding=\"utf-8\")\n",
    "            print(f\" -> Saved SINGLE table: {csv_path}\")\n",
    "            processed_tables_3.append(table)\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {name}: {e}\")\n",
    "\n",
    "if processed_tables_3:\n",
    "    final_table_3 = pd.concat(processed_tables_3, axis=1).fillna(0.0).round(2)\n",
    "    final_csv_3 = os.path.join(OUTPUT_DIR_3, \"final_presence_unified.csv\")\n",
    "    final_table_3.to_csv(final_csv_3, encoding=\"utf-8\")\n",
    "    print(f\"\\nFinal unified table saved to: {final_csv_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYSIS 4 ---\n",
    "OUTPUT_DIR_4 = \"results_nonzero\"\n",
    "METRICS_4 = [\"Num_addition\", \"Num_direct_modifications\"]\n",
    "\n",
    "def calculate_nonzero_median(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "    unique_pcts = sorted(df[\"percentage\"].dropna().unique())\n",
    "    manip_vals = sorted(df[\"manipulation\"].dropna().unique())\n",
    "    rows = []\n",
    "    for m in METRICS_4:\n",
    "        if m not in df.columns: continue\n",
    "        for pct in unique_pcts:\n",
    "            grouped = df[df[\"percentage\"] == pct].groupby(\"manipulation\")[m].apply(\n",
    "                lambda x: x[x > 0].median() if (x > 0).any() else 0.0\n",
    "            )\n",
    "            grouped = grouped.reindex(manip_vals, fill_value=0.0)\n",
    "            grouped.name = f\"{m}_{int(pct)}\"\n",
    "            rows.append(grouped)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    table = pd.concat(rows, axis=1).T\n",
    "    table.columns = [f\"{dataset_name}_{col}\" for col in table.columns]\n",
    "    return table.fillna(0.0).round(2)\n",
    "\n",
    "print(\"Starting Non-Zero Median Analysis...\")\n",
    "os.makedirs(OUTPUT_DIR_4, exist_ok=True)\n",
    "processed_tables_4 = []\n",
    "\n",
    "for name, path in FILES_CONFIG.items():\n",
    "    df = load_and_preprocess(path)\n",
    "    if df is not None:\n",
    "        try:\n",
    "            table = calculate_nonzero_median(df, name)\n",
    "            csv_path = os.path.join(OUTPUT_DIR_4, f\"{name}_nonzero_median.csv\")\n",
    "            table.to_csv(csv_path, encoding=\"utf-8\")\n",
    "            print(f\" -> Saved SINGLE table: {csv_path}\")\n",
    "            processed_tables_4.append(table)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name}: {e}\")\n",
    "\n",
    "if processed_tables_4:\n",
    "    final_table_4 = pd.concat(processed_tables_4, axis=1).fillna(0.0).round(2)\n",
    "    final_csv_4 = os.path.join(OUTPUT_DIR_4, \"final_nonzero_median_unified.csv\")\n",
    "    final_table_4.to_csv(final_csv_4, encoding=\"utf-8\")\n",
    "    print(f\"\\n Final unified table saved to: {final_csv_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b944de",
   "metadata": {},
   "source": [
    "VS BASELINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "files = {\n",
    "    \"GR\": \"gr_gpt_ALL.csv\",\n",
    "    \"SYNT\": \"synt_gpt_ALL.csv\",\n",
    "    \"READ\": \"read_gpt_ALL.csv\",\n",
    "    \"INFDENS\": \"infdens_gpt_ALL.csv\",\n",
    "}\n",
    "\n",
    "baseline_file = \"clapnqans_openai_answers_NEW.csv\"\n",
    "\n",
    "# === METRICS TO ANALYSE ===\n",
    "metrics_2 = [\n",
    "    \"acc_median_final\",\n",
    "    \"comp_final_median\",\n",
    "    \"key_concept_coverage\",\n",
    "    \"Conciseness\",\n",
    "]\n",
    "\n",
    "def build_variation_table(df, baseline_vals, df_name):\n",
    "    if \"percentage\" in df.columns:\n",
    "        group_col = \"percentage\"\n",
    "    elif \"setting\" in df.columns:  \n",
    "        mapping = {\"2s_3e\": 50, \"4s_3e\": 100}\n",
    "        df[\"percentage\"] = df[\"setting\"].map(mapping)\n",
    "        group_col = \"percentage\"\n",
    "    else:\n",
    "        raise ValueError(f\"❌ Nessuna colonna 'percentage' o 'setting' trovata in {df_name}\")\n",
    "\n",
    "    if \"manipulation\" not in df.columns:\n",
    "        raise ValueError(f\"❌ {df_name}: manca colonna 'manipulation'\")\n",
    "\n",
    "    grouped = df.groupby([\"manipulation\", group_col])[metrics_2].median()\n",
    "\n",
    "    rows = []\n",
    "    for m in metrics_2:\n",
    "        base_val = baseline_vals[m]  \n",
    "        for pct in sorted(df[group_col].dropna().unique()):\n",
    "            df_vals = grouped[m].xs(pct, level=group_col) if pct in grouped.index.get_level_values(group_col) else pd.Series()\n",
    "\n",
    "            variation = ((df_vals - base_val) / base_val * 100).round(2)\n",
    "            variation.name = f\"{m}_{int(pct)}\"\n",
    "            rows.append(variation)\n",
    "\n",
    "    if rows:\n",
    "        table = pd.concat(rows, axis=1).T\n",
    "        table.columns = [f\"{df_name}_{col}\" for col in table.columns]\n",
    "        table = table.fillna(0.0).round(2)\n",
    "    else:\n",
    "        table = pd.DataFrame(columns=[f\"{df_name}_{m}\" for m in grouped.index.get_level_values(\"manipulation\").unique()])\n",
    "\n",
    "    print(f\"\\n % variation from baseline for {df_name}:\")\n",
    "    print(tabulate(table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "\n",
    "    table.to_csv(f\"tabella_{df_name}_var_vs_baseline.csv\", encoding=\"utf-8\")\n",
    "    with open(f\"tabella_{df_name}_var_vs_baseline.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tabulate(table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "    print(f\"{df_name} saved in CSV e TXT\")\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "try:\n",
    "    baseline_df = pd.read_csv(baseline_file, encoding=\"utf-8\")\n",
    "    if \"sentence_percentage\" in baseline_df.columns:\n",
    "        baseline_df.rename(columns={\"sentence_percentage\": \"percentage\"}, inplace=True)\n",
    "\n",
    "    baseline_vals = baseline_df[metrics_2].median()\n",
    "    print(f\"baseline ({len(baseline_df)} rows)\")\n",
    "    print(\"baseline (median per metrics):\")\n",
    "    print(baseline_vals)\n",
    "\n",
    "    tables = []\n",
    "    for name, path in files.items():\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "            if \"sentence_percentage\" in df.columns:\n",
    "                df.rename(columns={\"sentence_percentage\": \"percentage\"}, inplace=True)\n",
    "            print(f\"Load {name} ({len(df)} rows)\")\n",
    "            t = build_variation_table(df, baseline_vals, name)\n",
    "            tables.append(t)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Errore con {name}: {e}\")\n",
    "\n",
    "    # === TABELLA FINALE ===\n",
    "    if tables:\n",
    "        final_table = pd.concat(tables, axis=1).fillna(0.0).round(2)\n",
    "\n",
    "        print(\"\\n Unified table (% variation from baseline):\")\n",
    "        print(tabulate(final_table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "\n",
    "        final_table.to_csv(\"tabella_finale_var_vs_baseline.csv\", encoding=\"utf-8\")\n",
    "        with open(\"tabella_finale_var_vs_baseline.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(tabulate(final_table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "        print(\"\\n Saved in CSV e TXT\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7fb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# === CONFIGURAZIONE ===\n",
    "files = {\n",
    "    \"GR\": \"gr_gpt_ALL.csv\",\n",
    "    \"SYNT\": \"synt_gpt_ALL.csv\",\n",
    "    \"READ\": \"read_gpt_ALL.csv\",\n",
    "    \"INFDENS\": \"infdens_gpt_ALL.csv\",\n",
    "}\n",
    "\n",
    "baseline_file = \"clapnqans_openai_answers_NEW.csv\"\n",
    "\n",
    "# === DUE SET DI SOGLIE ===\n",
    "thresholds_06 = {\n",
    "    \"acc_median_final\": 0.6,\n",
    "    \"comp_final_median\": 0.6,\n",
    "    \"key_concept_coverage\": 0.6,\n",
    "    \"Conciseness\": {\"low\": 0.5, \"high\": 1.5},\n",
    "    \"Num_addition\": 2,\n",
    "    \"Num_direct_modifications\": 2,\n",
    "}\n",
    "\n",
    "thresholds_04 = {\n",
    "    \"acc_median_final\": 0.4,\n",
    "    \"comp_final_median\": 0.4,\n",
    "    \"key_concept_coverage\": 0.4,\n",
    "    \"Num_addition\": 4,\n",
    "    \"Num_direct_modifications\": 4,\n",
    "}\n",
    "\n",
    "threshold_sets = {\n",
    "    \"thr06\": thresholds_06,\n",
    "    \"thr04\": thresholds_04,\n",
    "}\n",
    "\n",
    "def compute_baseline_thresholds(baseline_df, threshold_sets):\n",
    "    baseline_vals = {}\n",
    "    for thr_name, thresholds in threshold_sets.items():\n",
    "        for metric, thr in thresholds.items():\n",
    "            if metric == \"Conciseness\":\n",
    "                low, high = thr[\"low\"], thr[\"high\"]\n",
    "                below = float((baseline_df[metric] < low).sum() / len(baseline_df) * 100)\n",
    "                above = float((baseline_df[metric] > high).sum() / len(baseline_df) * 100)\n",
    "                baseline_vals[f\"{metric}below{low}\"] = below\n",
    "                baseline_vals[f\"{metric}above{high}\"] = above\n",
    "            elif metric in [\"Num_addition\", \"Num_direct_modifications\"]:\n",
    "                above = float((baseline_df[metric] > thr).sum() / len(baseline_df) * 100)\n",
    "                baseline_vals[f\"{metric}above{thr}\"] = above\n",
    "            else:\n",
    "                below = float((baseline_df[metric] < thr).sum() / len(baseline_df) * 100)\n",
    "                baseline_vals[f\"{metric}below{thr}\"] = below\n",
    "    return baseline_vals\n",
    "\n",
    "def build_thresholds_variation_table(df, df_name, threshold_sets, baseline_vals):\n",
    "    if \"sentence_percentage\" in df.columns:\n",
    "        df = df.rename(columns={\"sentence_percentage\": \"percentage\"})\n",
    "\n",
    "    if \"percentage\" not in df.columns:\n",
    "        if \"setting\" in df.columns:\n",
    "            mapping = {\"2s_3e\": 50, \"4s_3e\": 100}\n",
    "            df[\"percentage\"] = df[\"setting\"].map(mapping)\n",
    "        else:\n",
    "            raise ValueError(f\" {df_name}: no 'percentage' o 'setting'column\")\n",
    "\n",
    "    if \"manipulation\" not in df.columns:\n",
    "        raise ValueError(f\" {df_name}: no 'manipulation'column\")\n",
    "\n",
    "    unique_pcts = sorted(df[\"percentage\"].dropna().unique())\n",
    "    manip_vals = sorted(df[\"manipulation\"].dropna().unique())\n",
    "\n",
    "    rows = []\n",
    "    for thr_name, thresholds in threshold_sets.items():\n",
    "        for metric, thr in thresholds.items():\n",
    "            for pct in unique_pcts:\n",
    "                df_pct = df[df[\"percentage\"] == pct]\n",
    "                grouped = df_pct.groupby(\"manipulation\")\n",
    "\n",
    "                if metric == \"Conciseness\":\n",
    "                    low = thr[\"low\"]\n",
    "                    high = thr[\"high\"]\n",
    "\n",
    "                    s_below = grouped[metric].apply(\n",
    "                        lambda x: (x < low).sum() / x.count() * 100 if x.count() > 0 else 0.0\n",
    "                    ).reindex(manip_vals, fill_value=0.0)\n",
    "                    baseline_val = baseline_vals[f\"{metric}below{low}\"]\n",
    "                    variation_below = (s_below - baseline_val).round(2)\n",
    "                    variation_below.name = f\"{metric}below{low}_{int(pct)}\"\n",
    "\n",
    "                    s_above = grouped[metric].apply(\n",
    "                        lambda x: (x > high).sum() / x.count() * 100 if x.count() > 0 else 0.0\n",
    "                    ).reindex(manip_vals, fill_value=0.0)\n",
    "                    baseline_val = baseline_vals[f\"{metric}above{high}\"]\n",
    "                    variation_above = (s_above - baseline_val).round(2)\n",
    "                    variation_above.name = f\"{metric}above{high}_{int(pct)}\"\n",
    "\n",
    "                    rows.append(variation_below)\n",
    "                    rows.append(variation_above)\n",
    "\n",
    "                elif metric in [\"Num_addition\", \"Num_direct_modifications\"]:\n",
    "                    s_above = grouped[metric].apply(\n",
    "                        lambda x: (x > thr).sum() / x.count() * 100 if x.count() > 0 else 0.0\n",
    "                    ).reindex(manip_vals, fill_value=0.0)\n",
    "                    baseline_val = baseline_vals[f\"{metric}above{thr}\"]\n",
    "                    variation = (s_above - baseline_val).round(2)\n",
    "                    variation.name = f\"{metric}above{thr}_{int(pct)}\"\n",
    "                    rows.append(variation)\n",
    "\n",
    "                else:\n",
    "                    s_below = grouped[metric].apply(\n",
    "                        lambda x: (x < thr).sum() / x.count() * 100 if x.count() > 0 else 0.0\n",
    "                    ).reindex(manip_vals, fill_value=0.0)\n",
    "                    baseline_val = baseline_vals[f\"{metric}below{thr}\"]\n",
    "                    variation = (s_below - baseline_val).round(2)\n",
    "                    variation.name = f\"{metric}below{thr}_{int(pct)}\"\n",
    "                    rows.append(variation)\n",
    "\n",
    "    if rows:\n",
    "        table = pd.concat(rows, axis=1).T\n",
    "        table.columns = [f\"{df_name}_{col}\" for col in table.columns]\n",
    "        table = table.fillna(0.0).round(2)\n",
    "    else:\n",
    "        table = pd.DataFrame(columns=[f\"{df_name}_{m}\" for m in manip_vals])\n",
    "\n",
    "    print(f\"\\n Difference in points % below/above threshold compared to baseline for {df_name}:\")\n",
    "    print(tabulate(table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "\n",
    "    table.to_csv(f\"tabella_{df_name}_var_thresholds_vs_baseline.csv\", encoding=\"utf-8\")\n",
    "    with open(f\"tabella_{df_name}_var_thresholds_vs_baseline.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tabulate(table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "    print(f\" Saved {df_name} in CSV e TXT\")\n",
    "\n",
    "    return table\n",
    "\n",
    "try:\n",
    "    baseline_df = pd.read_csv(baseline_file, encoding=\"utf-8\")\n",
    "    baseline_vals = compute_baseline_thresholds(baseline_df, threshold_sets)\n",
    "    \n",
    "\n",
    "    tables = []\n",
    "    for name, path in files.items():\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "            print(f\"Load {name} ({len(df)} rows)\")\n",
    "            t = build_thresholds_variation_table(df, name, threshold_sets, baseline_vals)\n",
    "            tables.append(t)\n",
    "        except Exception as e:\n",
    "            print(f\" Errore with {name}: {e}\")\n",
    "\n",
    "    if tables:\n",
    "        final_table = pd.concat(tables, axis=1).fillna(0.0).round(2)\n",
    "\n",
    "        print(\"\\n Unified table (Difference in points percentage below/above threshold compared to baseline):\")\n",
    "        print(tabulate(final_table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "\n",
    "        final_table.to_csv(\"tabella_finale_var_thresholds_vs_baseline.csv\", encoding=\"utf-8\")\n",
    "        with open(\"tabella_finale_var_thresholds_vs_baseline.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(tabulate(final_table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "        print(\"\\n Unified table saved in CSV e TXT\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fa7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "files = {\n",
    "    \"GR\": \"gr_gpt_ALL.csv\",\n",
    "    \"SYNT\": \"synt_gpt_ALL.csv\",\n",
    "    \"READ\": \"read_gpt_ALL.csv\",\n",
    "    \"INFDENS\": \"infdens_gpt_ALL.csv\",\n",
    "}\n",
    "\n",
    "baseline_file = \"clapnqans_openai_answers_NEW.csv\"\n",
    "\n",
    "def compute_baseline_nonzero(baseline_df):\n",
    "    baseline_vals = {}\n",
    "    baseline_vals[\"%Additions\"] = (baseline_df[\"Num_addition\"] != 0).sum() / len(baseline_df) * 100\n",
    "    baseline_vals[\"%DirectModifications\"] = (baseline_df[\"Num_direct_modifications\"] != 0).sum() / len(baseline_df) * 100\n",
    "    return baseline_vals\n",
    "\n",
    "def build_nonzero_variation_table(df, df_name, baseline_vals):\n",
    "    if \"sentence_percentage\" in df.columns:\n",
    "        df = df.rename(columns={\"sentence_percentage\": \"percentage\"})\n",
    "    if \"percentage\" not in df.columns and \"setting\" in df.columns:\n",
    "        mapping = {\"2s_3e\": 50, \"4s_3e\": 100}\n",
    "        df[\"percentage\"] = df[\"setting\"].map(mapping)\n",
    "\n",
    "    if \"manipulation\" not in df.columns:\n",
    "        raise ValueError(f\" {df_name}: no 'manipulation' column\")\n",
    "\n",
    "    unique_pcts = sorted(df[\"percentage\"].dropna().unique())\n",
    "    manip_vals = sorted(df[\"manipulation\"].dropna().unique())\n",
    "\n",
    "    rows = []\n",
    "    for pct in unique_pcts:\n",
    "        df_pct = df[df[\"percentage\"] == pct]\n",
    "        grouped = df_pct.groupby(\"manipulation\")\n",
    "\n",
    "        # %Additions\n",
    "        s_add = grouped[\"Num_addition\"].apply(lambda x: (x != 0).sum() / x.count() * 100 if x.count() > 0 else 0.0).reindex(manip_vals, fill_value=0.0)\n",
    "        variation_add = (s_add - baseline_vals[\"%Additions\"]).round(2)\n",
    "        variation_add.name = f\"%Additions_{int(pct)}\"\n",
    "        rows.append(variation_add)\n",
    "\n",
    "        # %DirectModifications\n",
    "        s_dm = grouped[\"Num_direct_modifications\"].apply(lambda x: (x != 0).sum() / x.count() * 100 if x.count() > 0 else 0.0).reindex(manip_vals, fill_value=0.0)\n",
    "        variation_dm = (s_dm - baseline_vals[\"%DirectModifications\"]).round(2)\n",
    "        variation_dm.name = f\"%DirectModifications_{int(pct)}\"\n",
    "        rows.append(variation_dm)\n",
    "\n",
    "    if rows:\n",
    "        table = pd.concat(rows, axis=1).T\n",
    "        table.columns = [f\"{df_name}_{col}\" for col in table.columns]\n",
    "        table = table.fillna(0.0).round(2)\n",
    "    else:\n",
    "        table = pd.DataFrame(columns=[f\"{df_name}_{m}\" for m in manip_vals])\n",
    "\n",
    "    print(f\"\\n Difference in non-zero percentage points vs baseline for {df_name}:\")\n",
    "    print(tabulate(table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "\n",
    "    table.to_csv(f\"tabella_{df_name}_nonzero_vs_baseline.csv\", encoding=\"utf-8\")\n",
    "    with open(f\"tabella_{df_name}_nonzero_vs_baseline.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tabulate(table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "    print(f\" {df_name} saved in CSV e TXT\")\n",
    "\n",
    "    return table\n",
    "\n",
    "# === Baseline ===\n",
    "try:\n",
    "    baseline_df = pd.read_csv(baseline_file, encoding=\"utf-8\")\n",
    "    baseline_vals = compute_baseline_nonzero(baseline_df)\n",
    "    \n",
    "    tables = []\n",
    "    for name, path in files.items():\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "            print(f\"Load {name} ({len(df)} righe)\")\n",
    "            t = build_nonzero_variation_table(df, name, baseline_vals)\n",
    "            tables.append(t)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error with {name}: {e}\")\n",
    "\n",
    "    if tables:\n",
    "        final_table = pd.concat(tables, axis=1).fillna(0.0).round(2)\n",
    "\n",
    "        print(\"\\n Unified table (Difference in non-zero percentage points vs baseline):\")\n",
    "        print(tabulate(final_table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "\n",
    "        final_table.to_csv(\"tabella_finale_nonzero_vs_baseline.csv\", encoding=\"utf-8\")\n",
    "        with open(\"tabella_finale_nonzero_vs_baseline.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(tabulate(final_table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "        print(\"\\n Final table in CSV and TXT\")\n",
    "except FileNotFoundError:\n",
    "    print(f\" Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "files = {\n",
    "    \"GR\": \"gr_gpt_ALL.csv\",\n",
    "    \"SYNT\": \"synt_gpt_ALL.csv\",\n",
    "    \"READ\": \"read_gpt_ALL.csv\",\n",
    "    \"INFDENS\": \"infdens_gpt_ALL.csv\",\n",
    "}\n",
    "\n",
    "baseline_file = \"clapnqans_openai_answers_NEW.csv\"\n",
    "\n",
    "metrics = [\"Num_addition\"]\n",
    "\n",
    "def compute_baseline_median(baseline_df):\n",
    "    baseline_median = {}\n",
    "    for metric in metrics:\n",
    "        nonzero_vals = baseline_df[baseline_df[metric] != 0][metric]\n",
    "        baseline_median[metric] = nonzero_vals.median() if len(nonzero_vals) > 0 else 0.0\n",
    "    return baseline_median\n",
    "\n",
    "def build_nonzero_median_variation_table(df, df_name, baseline_median):\n",
    "    if \"sentence_percentage\" in df.columns:\n",
    "        df = df.rename(columns={\"sentence_percentage\": \"percentage\"})\n",
    "    if \"percentage\" not in df.columns and \"setting\" in df.columns:\n",
    "        mapping = {\"2s_3e\": 50, \"4s_3e\": 100}\n",
    "        df[\"percentage\"] = df[\"setting\"].map(mapping)\n",
    "\n",
    "    if \"manipulation\" not in df.columns:\n",
    "        raise ValueError(f\" {df_name}: no 'manipulation' column\")\n",
    "\n",
    "    unique_pcts = sorted(df[\"percentage\"].dropna().unique())\n",
    "    manip_vals = sorted(df[\"manipulation\"].dropna().unique())\n",
    "\n",
    "    rows = []\n",
    "    for metric in metrics:\n",
    "        for pct in unique_pcts:\n",
    "            df_pct = df[df[\"percentage\"] == pct]\n",
    "            grouped = df_pct.groupby(\"manipulation\")\n",
    "\n",
    "            s_median = grouped[metric].apply(lambda x: x[x != 0].median() if (x != 0).any() else 0.0).reindex(manip_vals, fill_value=0.0)\n",
    "\n",
    "            if baseline_median[metric] != 0:\n",
    "                variation = ((s_median - baseline_median[metric]) / baseline_median[metric] * 100).round(2)\n",
    "            else:\n",
    "                variation = s_median.round(2)\n",
    "\n",
    "            variation.name = f\"{metric}_{int(pct)}\"\n",
    "            rows.append(variation)\n",
    "\n",
    "    if rows:\n",
    "        table = pd.concat(rows, axis=1).T\n",
    "        table.columns = [f\"{df_name}_{col}\" for col in table.columns]\n",
    "        table = table.fillna(0.0).round(2)\n",
    "    else:\n",
    "        table = pd.DataFrame(columns=[f\"{df_name}_{m}\" for m in manip_vals])\n",
    "\n",
    "    print(f\"\\n Median non-zero % change vs baseline for {df_name}:\")\n",
    "    print(tabulate(table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "\n",
    "    table.to_csv(f\"tabella_{df_name}_median_nonzero_vs_baseline.csv\", encoding=\"utf-8\")\n",
    "    with open(f\"tabella_{df_name}_median_nonzero_vs_baseline.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tabulate(table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "    print(f\"  {df_name} saved in CSV e TXT\")\n",
    "\n",
    "    return table\n",
    "\n",
    "try:\n",
    "    baseline_df = pd.read_csv(baseline_file, encoding=\"utf-8\")\n",
    "    baseline_median = compute_baseline_median(baseline_df)\n",
    "    \n",
    "    tables = []\n",
    "    for name, path in files.items():\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "            print(f\"Load {name} ({len(df)} rows)\")\n",
    "            t = build_nonzero_median_variation_table(df, name, baseline_median)\n",
    "            tables.append(t)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error with {name}: {e}\")\n",
    "\n",
    "    if tables:\n",
    "        final_table = pd.concat(tables, axis=1).fillna(0.0).round(2)\n",
    "\n",
    "        print(\"\\n Unified table (Median non-zero percentage change vs baseline):\")\n",
    "        print(tabulate(final_table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "\n",
    "        final_table.to_csv(\"tabella_finale_median_nonzero_vs_baseline.csv\", encoding=\"utf-8\")\n",
    "        with open(\"tabella_finale_median_nonzero_vs_baseline.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(tabulate(final_table, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "        print(\"\\n Table saved in CSV e TXT\")\n",
    "except FileNotFoundError:\n",
    "    print(f\" Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3407dc",
   "metadata": {},
   "source": [
    "GPT vs GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# PATH AND FILE CONFIGURATION\n",
    "# ============================================================\n",
    "save_dir = r\"GPTvsGEMINI\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# GPT Datasets\n",
    "files_gpt = {\n",
    "    \"GR\": r\"gr_gpt_ALL.csv\",\n",
    "    \"SYNT\": r\"synt_gpt_ALL.csv\",\n",
    "    \"READ\": r\"read_gpt_ALL.csv\",\n",
    "    \"INFDENS\": r\"infdens_gpt_ALL.csv\",\n",
    "}\n",
    "\n",
    "# Gemini Datasets\n",
    "files_gemini = {\n",
    "    \"GR\": r\"gr_gemini_ALL.csv\",\n",
    "    \"SYNT\": r\"synt_gemini_ALL.csv\",\n",
    "    \"READ\": r\"read_gemini_ALL.csv\",\n",
    "    \"INFDENS\": r\"infdens_gemini_ALL.csv\",\n",
    "}\n",
    "\n",
    "# Baselines\n",
    "baseline_gpt_path = r\"clapnqans_openai_answers.csv\"\n",
    "baseline_gemini_path = r\"clapnqans_gemini_answers.csv\"\n",
    "\n",
    "# Metrics\n",
    "metrics = ['acc_median_final', 'comp_final_median', 'key_concept_coverage', 'Conciseness']\n",
    "metric_names = ['Accuracy', 'Completeness', 'KCC', 'Conciseness']\n",
    "\n",
    "# ============================================================\n",
    "# DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "def load_data_safe(files_dict):\n",
    "    loaded_data = {}\n",
    "    for key, path in files_dict.items():\n",
    "        if os.path.exists(path):\n",
    "            loaded_data[key] = pd.read_csv(path, encoding=\"utf-8\")\n",
    "        else:\n",
    "            print(f\"⚠ File not found: {path}\")\n",
    "    return loaded_data\n",
    "\n",
    "print(\"Loading GPT datasets...\")\n",
    "datasets_gpt = load_data_safe(files_gpt)\n",
    "\n",
    "print(\"Loading Gemini datasets...\")\n",
    "datasets_gemini = load_data_safe(files_gemini)\n",
    "\n",
    "# Combined structure for support functions\n",
    "datasets_combined = {}\n",
    "for key in datasets_gpt.keys():\n",
    "    if key in datasets_gemini:\n",
    "        datasets_combined[key] = {\n",
    "            \"GPT\": datasets_gpt[key],\n",
    "            \"Gemini\": datasets_gemini[key]\n",
    "        }\n",
    "\n",
    "# Loading Baselines\n",
    "try:\n",
    "    baseline_df_gpt = pd.read_csv(baseline_gpt_path, encoding=\"utf-8\")\n",
    "    baseline_df_gemini = pd.read_csv(baseline_gemini_path, encoding=\"utf-8\")\n",
    "    print(\"Baselines loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading baselines: {e}\")\n",
    "    baseline_df_gpt = pd.DataFrame() # Empty fallback\n",
    "    baseline_df_gemini = pd.DataFrame()\n",
    "\n",
    "# ============================================================\n",
    "# SUPPORT FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def collect_all_manipulations(datasets, model_name):\n",
    "    all_manips = set()\n",
    "    for dim, dfs in datasets.items():\n",
    "        if model_name in dfs:\n",
    "            df = dfs[model_name]\n",
    "            if \"manipulation\" in df.columns:\n",
    "                all_manips.update(df['manipulation'].unique())\n",
    "    return sorted(all_manips)\n",
    "\n",
    "def calculate_baseline_medians(baseline_df):\n",
    "    if baseline_df.empty: return {mname: 0 for mname in metric_names}\n",
    "    return {mname: baseline_df[mcol].dropna().median()\n",
    "            for mcol, mname in zip(metrics, metric_names)}\n",
    "\n",
    "def collect_manipulation_medians(datasets, model_name, baseline_medians):\n",
    "    all_manips = collect_all_manipulations(datasets, model_name)\n",
    "    results = []\n",
    "\n",
    "    for manip in all_manips:\n",
    "        row = {\"Manipulation\": manip}\n",
    "        for mcol, mname in zip(metrics, metric_names):\n",
    "            baseline = baseline_medians[mname]\n",
    "            for pct in [50, 100]:\n",
    "                values = []\n",
    "                for dim, dfs in datasets.items():\n",
    "                    if model_name not in dfs: continue\n",
    "                    df = dfs[model_name].copy()\n",
    "\n",
    "                    if \"sentence_percentage\" in df.columns:\n",
    "                        df.rename(columns={\"sentence_percentage\": \"percentage\"}, inplace=True)\n",
    "                    if \"setting\" in df.columns and dim == \"INFDENS\":\n",
    "                        mapping = {\"2s_3e\": 50, \"4s_3e\": 100}\n",
    "                        df[\"percentage\"] = df[\"setting\"].map(mapping)\n",
    "                    \n",
    "                    if \"percentage\" in df.columns and \"manipulation\" in df.columns:\n",
    "                        vals = df[(df[\"manipulation\"] == manip) & (df[\"percentage\"] == pct)][mcol].dropna().tolist()\n",
    "                        values.extend(vals)\n",
    "\n",
    "                if values:\n",
    "                    med = np.median(values)\n",
    "                    row[f\"{mname}_{pct}%\"] = med\n",
    "                    row[f\"{mname}_{pct}%-Base\"] = med - baseline\n",
    "                else:\n",
    "                    row[f\"{mname}_{pct}%\"] = np.nan\n",
    "                    row[f\"{mname}_{pct}%-Base\"] = np.nan\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "if not baseline_df_gpt.empty and not baseline_df_gemini.empty and datasets_combined:\n",
    "    # 1. Calculate Baseline Medians\n",
    "    baseline_medians_gpt_vals = calculate_baseline_medians(baseline_df_gpt)\n",
    "    baseline_medians_gemini_vals = calculate_baseline_medians(baseline_df_gemini)\n",
    "\n",
    "    # 2. Calculate Medians by Manipulation\n",
    "    df_gpt_res = collect_manipulation_medians(datasets_combined, \"GPT\", baseline_medians_gpt_vals)\n",
    "    df_gemini_res = collect_manipulation_medians(datasets_combined, \"Gemini\", baseline_medians_gemini_vals)\n",
    "\n",
    "\n",
    "    def create_full_cumulative(df_gpt, df_gemini):\n",
    "        rows = []\n",
    "        for _, row_gpt in df_gpt.iterrows():\n",
    "            manip = row_gpt[\"Manipulation\"]\n",
    "            row_gemini = df_gemini[df_gemini[\"Manipulation\"] == manip]\n",
    "            if len(row_gemini) == 0:\n",
    "                continue\n",
    "            row_gemini = row_gemini.iloc[0]\n",
    "            for pct in [50, 100]:\n",
    "                r = {\"Manipulation\": manip, \"Percentage\": pct}\n",
    "                for mname in metric_names:\n",
    "                    gpt_base = row_gpt[f\"{mname}_{pct}%-Base\"]\n",
    "                    gem_base = row_gemini[f\"{mname}_{pct}%-Base\"]\n",
    "                    \n",
    "                    r[f\"{mname}_GPT%-Base\"] = gpt_base\n",
    "                    r[f\"{mname}_Gemini%-Base\"] = gem_base\n",
    "                    r[f\"{mname}_Δ\"] = gpt_base - gem_base\n",
    "                rows.append(r)\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    cumulative_full = create_full_cumulative(df_gpt_res, df_gemini_res)\n",
    "    \n",
    "    # Save Result\n",
    "    cumulative_path = os.path.join(save_dir, \"Cumulative_GPT_vs_Gemini_FULL.csv\")\n",
    "    cumulative_full.to_csv(cumulative_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"\\n  cumulative table saved to: {cumulative_path}\")\n",
    "    \n",
    "    # Preview\n",
    "    print(\"Cumulative Table Preview:\")\n",
    "    print(cumulative_full.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "else:\n",
    "    print(\"Cannot proceed: Missing Data or Baselines.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
