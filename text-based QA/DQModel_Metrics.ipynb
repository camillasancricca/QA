{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8963ac7d",
   "metadata": {},
   "source": [
    "LIBRERIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e747689",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bad060",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd251633",
   "metadata": {},
   "source": [
    "GRAMATICALLY / SYNTAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eee331",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6257990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a529cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3072b",
   "metadata": {},
   "source": [
    "READABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcb8fb",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac566d23",
   "metadata": {},
   "source": [
    "FACTUALITY/INFDENSITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17659b3b",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e276c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24622db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941523b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factuality\n",
    "def check_fact_quantified(claim, context, max_length=512):\n",
    "    inputs = nli_tokenizer(\n",
    "        context,\n",
    "        claim,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    outputs = nli_model_raw(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = logits.softmax(dim=-1).squeeze()\n",
    "    \n",
    "    contradiction_index = nli_model_raw.config.label2id.get(\"CONTRADICTION\", 2)  \n",
    "    return probs[contradiction_index].item()\n",
    "\n",
    "# Information Density\n",
    "def information_density(text):\n",
    "    doc = nlp(text)\n",
    "    return len(doc.ents) / len(doc) if len(doc) > 0 else 0.0\n",
    "\n",
    "#Information Relevance\n",
    "def information_relevance(question, text):\n",
    "    embeddings = sbert_model.encode([question, text])\n",
    "    return util.cos_sim(embeddings[0], embeddings[1]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542522b8",
   "metadata": {},
   "source": [
    "COHERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd83e4",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Local coherence\n",
    "def compute_local_coherence(text):\n",
    "    sentences = sent_tokenize(text)  \n",
    "    if len(sentences) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute Embeddings\n",
    "    embeddings = embedding_model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Similarity Matrix\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "    local_scores = [cosine_scores[i][i + 1].item() for i in range(len(sentences) - 1)]\n",
    "\n",
    "    return round(np.mean(local_scores), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9b7e7",
   "metadata": {},
   "source": [
    "SENTENCE FLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01eb8ae",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c365423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Normalized order score (Kendall tau)\n",
    "def normalized_order_score(original_sentences, modified_sentences):\n",
    "\n",
    "    if len(original_sentences) != len(modified_sentences):\n",
    "        raise ValueError(\"Different number of sentences\")\n",
    "\n",
    "    # Map each original sentence to its position\n",
    "    position_map = {s: i for i, s in enumerate(original_sentences)}\n",
    "    \n",
    "    try:\n",
    "        modified_indices = [position_map[s] for s in modified_sentences]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"A modified sentence is not present in the original\")\n",
    "\n",
    "    # Compute Kendall tau correlation\n",
    "    tau, _ = kendalltau(range(len(original_sentences)), modified_indices)\n",
    "    print(tau)\n",
    "    \n",
    "    if tau is None:\n",
    "        return 0.0  # maximum disorder in case of error\n",
    "\n",
    "    # Normalize to [0,1], 1 = perfect order, 0 = reverse order\n",
    "    return (tau + 1) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#Semantic Flow\n",
    "def semantic_coherence_flow(sentences):\n",
    "\n",
    "    if len(sentences) < 2:\n",
    "        return 1.0  \n",
    "\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    similarities = [\n",
    "        util.cos_sim(embeddings[i], embeddings[i + 1]).item()\n",
    "        for i in range(len(sentences) - 1)\n",
    "    ]\n",
    "\n",
    "    return sum(similarities) / len(similarities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
