{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b724593",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c065cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6da060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI CALL\n",
    "def ask_openai(prompt, model, api_key):\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42df36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Modified main function\n",
    "def extract_passage_triples(text, api_key, model=\"gpt-4\"):\n",
    "\n",
    "    # Prompt for the entire passage\n",
    "    prompt = f\"\"\"You are an expert in natural language processing and knowledge extraction.\n",
    "    Your task is to extract concise and meaningful [\"subject\", \"predicate\", \"object\"] triples from the following passage.\n",
    "\n",
    "    Instructions:\n",
    "    - Each triple must represent a core idea using only key terms (avoid function words or excessive detail).\n",
    "    - Format: [Subject, Predicate, Object].\n",
    "    - Use concise keywords or noun phrases for the subject and object.\n",
    "    - Use the main verb or verbal phrase as the predicate.\n",
    "    - Do not repeat triples that convey the same meaning.\n",
    "    - Do not add explanations or extra text.\n",
    "    - Enclose locutions (titles, citations, etc.) in quotation marks (\" \") since they are important concepts.\n",
    "\n",
    "    Extract the triples from this passage: {text}\"\"\"\n",
    "\n",
    "    triple = ask_openai(prompt, model, api_key)\n",
    "\n",
    "    results = [{\n",
    "        \"sentence\": text,\n",
    "        \"triples\": triple.strip()\n",
    "    }]\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c05445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "import spacy\n",
    "import requests\n",
    "import random\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ======= Extract meaningful words ======= #\n",
    "def extract_meaningful_words(results, original_text):\n",
    "    final_words = set()\n",
    "    \n",
    "    doc_original = nlp(original_text)\n",
    "\n",
    "    # Count real words in the text (exclude punctuation, spaces)\n",
    "    total_text_words = [\n",
    "        token for token in doc_original\n",
    "        if not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    total_words = len(total_text_words)\n",
    "\n",
    "    # Entities\n",
    "    for ent in doc_original.ents:\n",
    "        ent_doc = nlp(ent.text)\n",
    "        for token in ent_doc:\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                final_words.add(token.text.lower())  # <--- use token.text, not lemma_\n",
    "\n",
    "    # Words from triples (only NOUN, PRON, VERB, ADJ)\n",
    "    for r in results:\n",
    "        triple_doc = nlp(r[\"triple\"])\n",
    "        for token in triple_doc:\n",
    "            if token.pos_ in {\"NOUN\", \"PRON\", \"VERB\", \"ADJ\", \"ADV\", \"PROPN\"}:\n",
    "                if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                    final_words.add(token.text.lower())\n",
    "\n",
    "    extracted_words = list(final_words)\n",
    "    extracted_count = len(extracted_words)\n",
    "    percentage = (extracted_count / total_words * 100) if total_words > 0 else 0\n",
    "\n",
    "    return extracted_words, extracted_count, total_words, percentage\n",
    "\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"clapnqans.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "api_key = API_KEY\n",
    "final_results = []\n",
    "i = 0\n",
    "for item in data:\n",
    "    passage_id = item[\"id\"]\n",
    "    passage_text = item[\"passages\"][0][\"text\"]\n",
    "    text = fix_text(passage_text)\n",
    "\n",
    "    results = extract_passage_triples(text, api_key)\n",
    "\n",
    "    words, n_words, n_text, percentage = extract_meaningful_words(results, text)\n",
    "\n",
    "    if percentage <= 45:\n",
    "        # Already OK\n",
    "        final_results.append({\n",
    "            \"id\": passage_id,\n",
    "            \"text\": text,\n",
    "            \"triple\": results[0][\"triple\"] if results else \"FAILED\",\n",
    "            \"words\": words,\n",
    "            \"n_words\": n_words,\n",
    "            \"n_text\": n_text,\n",
    "            \"percentage\": round(percentage, 2)\n",
    "        })\n",
    "    else:\n",
    "\n",
    "        # Calculate how many words to keep to respect max 45%\n",
    "        max_allowed_words = int(n_text * 0.45)\n",
    "\n",
    "        if len(words) > max_allowed_words:\n",
    "            words = random.sample(words, max_allowed_words)\n",
    "\n",
    "        final_results.append({\n",
    "            \"id\": passage_id,\n",
    "            \"text\": text,\n",
    "            \"triple\": results[0][\"triple\"] if results else \"FAILED\",\n",
    "            \"words\": words,\n",
    "            \"n_words\": len(words),\n",
    "            \"n_text\": n_text,\n",
    "            \"percentage\": round((len(words) / n_text * 100), 2) if n_text > 0 else 0\n",
    "        })\n",
    "\n",
    "# ======= Save to CSV ======= #\n",
    "df = pd.DataFrame(final_results)\n",
    "df.to_csv(\"words_to_modify.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
