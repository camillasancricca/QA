{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8963ac7d",
   "metadata": {},
   "source": [
    "LIBRERIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e747689",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bad060",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17456e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb2f00",
   "metadata": {},
   "source": [
    "OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e07de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86908d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_answer_rag_openai(question, context=\"\"):\n",
    "    prompt = (\n",
    "        \"You must answer the question only and exclusively based on the provided context, \"\n",
    "        \"ignoring any prior knowledge or assumptions. \"\n",
    "        \"Your answer must be detailed, exhaustive and complete. \"\n",
    "        \"Do not explicitly cite the text.\\n\\n\"\n",
    "        f\"Context: {context}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",   \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers strictly based on the context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd251633",
   "metadata": {},
   "source": [
    "GRAMATICALLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d756f0",
   "metadata": {},
   "source": [
    "Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2277108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "boundary_punctuation = '.,!?;:()[]{}\"\\'“”‘’--'\n",
    "\n",
    "# ===== swap ===== #\n",
    "def swap_letters(word):\n",
    "\n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    \n",
    "    # Extract prefix punctuation\n",
    "    while len(word) > 0 and word[0] in boundary_punctuation:\n",
    "        prefix += word[0]\n",
    "        word = word[1:]\n",
    "    \n",
    "    # Extract suffix punctuation\n",
    "    while len(word) > 0 and word[-1] in boundary_punctuation:\n",
    "        suffix = word[-1] + suffix\n",
    "        word = word[:-1]\n",
    "    \n",
    "    # If too short, return unchanged\n",
    "    if len(word) < 4:\n",
    "        return prefix + word + suffix\n",
    "    \n",
    "    # Swap middle letters\n",
    "    middle = list(word[1:-1])\n",
    "    if len(middle) < 2:\n",
    "        return prefix + word + suffix\n",
    "\n",
    "    idx = random.randint(0, len(middle) - 2)\n",
    "    middle[idx], middle[idx + 1] = middle[idx + 1], middle[idx]\n",
    "    \n",
    "    swapped = word[0] + ''.join(middle) + word[-1]\n",
    "    return prefix + swapped + suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "boundary_punctuation = \".,;:!?()[]{}\\\"'\"\n",
    "\n",
    "# QWERTY keyboard neighbors map\n",
    "keyboard_neighbors = {\n",
    "    'a': 'qwsz', 'b': 'vghn', 'c': 'xdfv', 'd': 'serfcx',\n",
    "    'e': 'wsdr', 'f': 'drtgcv', 'g': 'ftyhbv', 'h': 'gyujnb',\n",
    "    'i': 'ujko', 'j': 'huikmn', 'k': 'jiolm', 'l': 'kop',\n",
    "    'm': 'njk', 'n': 'bhjm', 'o': 'iklp', 'p': 'ol',\n",
    "    'q': 'wa', 'r': 'edft', 's': 'awedxz', 't': 'rfgy',\n",
    "    'u': 'yhji', 'v': 'cfgb', 'w': 'qase', 'x': 'zsdc',\n",
    "    'y': 'tghu', 'z': 'asx',\n",
    "    '0': '9', '1': '2q', '2': '13w', '3': '24e', '4': '35r',\n",
    "    '5': '46t', '6': '57y', '7': '68u', '8': '79i', '9': '80o',\n",
    "    '+': '=-*ì',\n",
    "    '=': '-+',\n",
    "    '-': '=0_'\n",
    "}\n",
    "\n",
    "# ===== keyboard typo ===== #\n",
    "def keyboard_typo(word):\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    \n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    \n",
    "    while len(word) > 0 and word[0] in boundary_punctuation:\n",
    "        prefix += word[0]\n",
    "        word = word[1:]\n",
    "\n",
    "    while len(word) > 0 and word[-1] in boundary_punctuation:\n",
    "        suffix = word[-1] + suffix\n",
    "        word = word[:-1]\n",
    "\n",
    "    if len(word) == 0:\n",
    "        return prefix + suffix\n",
    "\n",
    "    # Generate the typo\n",
    "    idx = random.randint(0, len(word) - 1)\n",
    "    char = word[idx].lower()\n",
    "\n",
    "    if char in keyboard_neighbors:\n",
    "        replacement = random.choice(keyboard_neighbors[char])\n",
    "        word = word[:idx] + replacement + word[idx + 1:]\n",
    "\n",
    "    return prefix + word + suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Remove ===== #\n",
    "def remove_letters(word, max_remove=3):\n",
    "\n",
    "    # Decide how many letters to remove (between 1 and 'max_remove')\n",
    "    num_to_remove = random.randint(1, max_remove)\n",
    "\n",
    "    word_list = list(word)\n",
    "    \n",
    "    for _ in range(num_to_remove):\n",
    "        if word_list: \n",
    "            index_to_remove = random.randint(0, len(word_list) - 1)\n",
    "            word_list.pop(index_to_remove)\n",
    "\n",
    "    return ''.join(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "boundary_punctuation = '.,!?;:\"\\'()[]{}'\n",
    "\n",
    "# ===== Random typo ===== #\n",
    "def random_typo(word):\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    \n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "\n",
    "    while len(word) > 0 and word[0] in boundary_punctuation:\n",
    "        prefix += word[0]\n",
    "        word = word[1:]\n",
    "\n",
    "    while len(word) > 0 and word[-1] in boundary_punctuation:\n",
    "        suffix = word[-1] + suffix\n",
    "        word = word[:-1]\n",
    "    \n",
    "    if len(word) == 0:\n",
    "        return prefix + suffix\n",
    "\n",
    "    # Number of letters to replace (between 1 and 3)\n",
    "    num_replacements = random.randint(1, min(3, len(word)))\n",
    "\n",
    "    # Select unique positions in the word\n",
    "    positions = random.sample(range(len(word)), num_replacements)\n",
    "    word_chars = list(word)\n",
    "\n",
    "    for idx in positions:\n",
    "        random_char = random.choice(string.ascii_letters + string.digits)\n",
    "        word_chars[idx] = random_char\n",
    "\n",
    "    return prefix + ''.join(word_chars) + suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punctuation(word):\n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    \n",
    "    # Separa la punteggiatura all'inizio della parola\n",
    "    while len(word) > 0 and word[0] in boundary_punctuation:\n",
    "        prefix += word[0]\n",
    "        word = word[1:]\n",
    "    \n",
    "    # Separa la punteggiatura alla fine della parola\n",
    "    while len(word) > 0 and word[-1] in boundary_punctuation:\n",
    "        suffix = word[-1] + suffix\n",
    "        word = word[:-1]\n",
    "    \n",
    "    return prefix, word, suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08866338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def modify_all_words(text, words_to_modify, modification_type=\"swap\"):\n",
    "    modified_words = []  # List to keep track of modified words\n",
    "    \n",
    "    # Function to modify a word based on the modification type\n",
    "    def modify_word(word):\n",
    "        if modification_type == \"swap\":\n",
    "            return swap_letters(word)\n",
    "        elif modification_type == \"key_typo\":\n",
    "            return keyboard_typo(word)\n",
    "        elif modification_type == \"remove\":\n",
    "            return remove_letters(word, 3)\n",
    "        elif modification_type == \"ran_typo\":\n",
    "            return random_typo(word)\n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # List of modified words in the text\n",
    "    modified_text = \" \".join([\n",
    "        modify_word(word) if separate_punctuation(word)[1].lower() in [w.lower() for w in words_to_modify] else word\n",
    "        for word in words\n",
    "    ])\n",
    "    \n",
    "    # Add modified words to the tracking list\n",
    "    for word in words:\n",
    "        clean_word = separate_punctuation(word)[1].lower()\n",
    "        if clean_word in [w.lower() for w in words_to_modify] and word not in modified_words:\n",
    "            modified_words.append(separate_punctuation(word)[1])\n",
    "    \n",
    "    return modified_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eee331",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6257990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c4b8b",
   "metadata": {},
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulations_gr = {\n",
    "    \"swap\": modify_all_words,\n",
    "    \"key_typo\": modify_all_words,\n",
    "    \"remove\": modify_all_words,\n",
    "    \"ran_typo\": modify_all_words,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import evaluate\n",
    "\n",
    "# Metriche\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def run_pipeline_gr(text, question, words_to_modify, answer, anw_sentences, output_csv_path, manipulation_functions, runs_per_condition=4):\n",
    "    words = text.split()\n",
    "    results = []      \n",
    "\n",
    "    if output_csv_path is not None:\n",
    "        file = open(output_csv_path, mode=\"w\", newline='', encoding='utf-8')\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            \"timestamp\", \"manipulation\", \"percentage\", \"run\",\n",
    "            \"WER\", \"CER\", \"modified_words\",  \"modified_text\", \"openai_answer\",\n",
    "        ])\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    for manipulation_name, func in manipulation_functions.items():\n",
    "        for percentage in [50, 100]:\n",
    "            for run in range(1, runs_per_condition + 1):\n",
    "                num_words_to_change = len(words_to_modify) * percentage // 100\n",
    "                if num_words_to_change > 0:\n",
    "                    selected_words = random.sample(words_to_modify, num_words_to_change)\n",
    "                else:\n",
    "                    selected_words = []\n",
    "\n",
    "                modified_text = func(text, selected_words, manipulation_name)\n",
    "\n",
    "                wer = wer_metric.compute(predictions=[modified_text], references=[text])\n",
    "                cer = cer_metric.compute(predictions=[modified_text], references=[text])\n",
    "\n",
    "\n",
    "                openai_answer = generate_answer_rag_openai(question, modified_text)\n",
    "\n",
    "                # Results\n",
    "                result_dict = {\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"manipulation\": manipulation_name,\n",
    "                    \"percentage\": percentage,\n",
    "                    \"run\": run,\n",
    "                    \"WER\": round(wer, 4),\n",
    "                    \"CER\": round(cer, 4),\n",
    "                    \"modified_words\": \", \".join(selected_words),\n",
    "                    \"modified_text\": modified_text,\n",
    "                    \"openai_answer\": openai_answer.strip()\n",
    "                }\n",
    "\n",
    "                if writer is not None:\n",
    "                    writer.writerow(list(result_dict.values()))\n",
    "                else:\n",
    "                    results.append(result_dict)\n",
    "\n",
    "    if writer is not None:\n",
    "        file.close()\n",
    "\n",
    "    return results if writer is None else {\"status\": \"saved_to_file\", \"path\": output_csv_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3072b",
   "metadata": {},
   "source": [
    "READABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7e02a",
   "metadata": {},
   "source": [
    "Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ee23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#=========  Complex Synonyms  ======== #\n",
    "def sub_with_harder_synonyms(text, words):\n",
    "    for word in words:\n",
    "        synsets = wn.synsets(word)\n",
    "        synonyms = {lemma.name() for s in synsets for lemma in s.lemmas() if lemma.name().lower() != word.lower()}\n",
    "        longer_syns = [syn for syn in synonyms if len(syn) > len(word)]\n",
    "        if longer_syns:\n",
    "            harder_syn = sorted(longer_syns, key=len, reverse=True)[0].replace('_', ' ')\n",
    "            text = re.sub(r'\\b{}\\b'.format(re.escape(word)), harder_syn, text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Definition Insertion ======== #\n",
    "def add_definitions_to_words(text, words):\n",
    "    for word in words:\n",
    "        synsets = wn.synsets(word)\n",
    "        if synsets:\n",
    "            definition = synsets[0].definition()\n",
    "            text = re.sub(r'\\b{}\\b'.format(re.escape(word)), f\"{word} ({definition})\", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Punctuation Removal ======== #\n",
    "def remove_or_modify_punctuation(text,words_to_change):\n",
    "    text = re.sub(r'[.,;!?]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef30381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Duplicate Words ======== #\n",
    "def duplicate_words(text, words_to_duplicate):\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    for w in words:\n",
    "        result.append(w)\n",
    "   \n",
    "        if w.lower() in [word.lower() for word in words_to_duplicate]:\n",
    "            result.append(w)\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e28aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "#========= Double Negation ======== #\n",
    "def double_negate_sentence_logically(sentence):\n",
    "    \"\"\"\n",
    "    Applica una doppia negazione a una frase, scegliendo in modo casuale tra più varianti.\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    if not sentence:\n",
    "        return sentence\n",
    "\n",
    "    if sentence[-1] not in '.!?':\n",
    "        sentence += '.'\n",
    "\n",
    "    sentence_core = sentence[0].lower() + sentence[1:-1]  \n",
    "\n",
    "    # Templates for double negation\n",
    "    templates = [\n",
    "        f\"It is not the case that {sentence_core} is not true.\",\n",
    "        f\"It cannot be said that {sentence_core} is not correct.\",\n",
    "        f\"There is no way that {sentence_core} is not valid.\",\n",
    "        f\"It would be false to claim that {sentence_core} is not true.\",\n",
    "        f\"It is not false that {sentence_core}.\",\n",
    "        f\"It is not untrue that {sentence_core}.\",\n",
    "    ]\n",
    "\n",
    "    return random.choice(templates)\n",
    "\n",
    "def double_negation_by_sentence(text, fraction, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    total = len(sentences)\n",
    "    n_to_change = round(fraction * total)\n",
    "\n",
    "    indices_to_change = set(random.sample(range(total), n_to_change))\n",
    "\n",
    "    transformed = [\n",
    "        double_negate_sentence_logically(s) if i in indices_to_change else s\n",
    "        for i, s in enumerate(sentences)\n",
    "    ]\n",
    "    return ' '.join(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcb8fb",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2cedd",
   "metadata": {},
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulations_read = {\n",
    "    \"complex_synonyms\": sub_with_harder_synonyms,\n",
    "    \"definition_insertion\": add_definitions_to_words,\n",
    "    \"double_negation\": None \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from datetime import datetime\n",
    "import textstat  \n",
    "import time\n",
    "\n",
    "def run_pipeline_read(text, question, words_to_modify, answer, anw_sentences, output_csv_path, manipulation_functions, runs_per_condition=4):\n",
    "    words = text.split()\n",
    "    results = []    \n",
    "\n",
    "    if output_csv_path is not None:\n",
    "        file = open(output_csv_path, mode=\"w\", newline='', encoding='utf-8')\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            \"timestamp\", \"manipulation\", \"percentage\", \"run\",\n",
    "            \"Flesch_Reading_Ease\", \"Flesch_Kincaid_Grade\", \"Gunning_Fog_Index\",\n",
    "            \"modified_words\", \"modified_text\", \"openai_answer\"\n",
    "        ])\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    for manipulation_name, func in manipulation_functions.items():\n",
    "        for percentage in [50, 100]:\n",
    "            for run in range(1, runs_per_condition + 1):\n",
    "                \n",
    "                # Gestione manipolazioni speciali\n",
    "                if manipulation_name == \"double_negation\":\n",
    "                    fraction = percentage / 100.0\n",
    "                    modified_text = double_negation_by_sentence(text, fraction)\n",
    "                    selected_words = []  # non usato\n",
    "                else:\n",
    "                    num_words_to_change = len(words_to_modify) * percentage // 100\n",
    "                    selected_words = random.sample(words_to_modify, num_words_to_change) if num_words_to_change > 0 else []\n",
    "                    modified_text = func(text, selected_words)\n",
    "\n",
    "                fre = textstat.flesch_reading_ease(modified_text)\n",
    "                fkgl = textstat.flesch_kincaid_grade(modified_text)\n",
    "                gfi = textstat.gunning_fog(modified_text)\"\n",
    "\n",
    "                openai_answer = generate_answer_rag_openai(question, modified_text)\n",
    "\n",
    "                result_dict = {\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"manipulation\": manipulation_name,\n",
    "                    \"percentage\": percentage,\n",
    "                    \"run\": run,\n",
    "                    \"Flesch_Reading_Ease\": round(fre, 2),\n",
    "                    \"Flesch_Kincaid_Grade\": round(fkgl, 2),\n",
    "                    \"Gunning_Fog_Index\": round(gfi, 2),\n",
    "                    \"modified_words\": \", \".join(selected_words),\n",
    "                    \"modified_text\": modified_text,\n",
    "                    \"openai_answer\": openai_answer.strip()\n",
    "                }\n",
    "\n",
    "                if writer is not None:\n",
    "                    writer.writerow(list(result_dict.values()))\n",
    "                else:\n",
    "                    results.append(result_dict)\n",
    "\n",
    "    if writer is not None:\n",
    "        file.close()\n",
    "\n",
    "    return results if writer is None else {\"status\": \"saved_to_file\", \"path\": output_csv_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0741247",
   "metadata": {},
   "source": [
    "SYNTATICALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyinflect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5233f1",
   "metadata": {},
   "source": [
    "Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77f896a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from copy import deepcopy\n",
    "\n",
    "# Carica modello spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Utility\n",
    "def get_sentences(text):\n",
    "    return [sent.text.strip() for sent in nlp(text).sents]\n",
    "\n",
    "def get_tokens_with_pos(sentence):\n",
    "    return [(token.text, token.pos_, token) for token in nlp(sentence)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c222f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyinflect\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_lemma(word):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "    return token._.inflect(\"VB\") or token.lemma_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Verb tense errors ===== #\n",
    "def alter_verb_tense(sentence, inside_percentage=100):\n",
    "    # Get tokens with part-of-speech tags\n",
    "    tokens = get_tokens_with_pos(sentence)\n",
    "    words = [w for w, _, _ in tokens]\n",
    "    \n",
    "    verb_indices = [i for i, (_, pos, _) in enumerate(tokens) if pos in [\"AUX\", \"VERB\"]]\n",
    "\n",
    "    n = int(len(verb_indices) * inside_percentage / 100)\n",
    "    indices_to_change = random.sample(verb_indices, n) if n < len(verb_indices) else verb_indices\n",
    "\n",
    "    for i in indices_to_change:\n",
    "        word, pos, token = tokens[i]\n",
    "        lower_word = word.lower()\n",
    "\n",
    "        # Concord errors: is/are, was/were\n",
    "        if lower_word == \"is\":\n",
    "            words[i] = \"are\"\n",
    "        elif lower_word == \"are\":\n",
    "            words[i] = \"is\"\n",
    "        elif lower_word == \"was\":\n",
    "            words[i] = \"were\"\n",
    "        elif lower_word == \"were\":\n",
    "            words[i] = \"was\"\n",
    "        else:\n",
    "            # Change verb tense (e.g., past -> base)\n",
    "            lemma = token.lemma_\n",
    "            if token.tag_ in [\"VBD\", \"VBN\"]:  # past\n",
    "                new_form = token._.inflect(\"VB\")  # base form\n",
    "            else:  # present or base\n",
    "                new_form = token._.inflect(\"VBD\")  # past form\n",
    "\n",
    "            if new_form:\n",
    "                words[i] = new_form\n",
    "            else:\n",
    "                words[i] = lemma \n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff15027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Remove Subject/Verb ===== #\n",
    "def remove_tokens_by_type(sentence, remove_type='verb'):\n",
    "    tokens = get_tokens_with_pos(sentence)\n",
    "\n",
    "    if remove_type == 'verb':\n",
    "\n",
    "        filtered_words = [\n",
    "            word for word, pos, _ in tokens\n",
    "            if pos not in ['VERB', 'AUX']\n",
    "        ]\n",
    "    elif remove_type == 'noun':\n",
    "\n",
    "        filtered_words = [\n",
    "            word for word, pos, tok in tokens\n",
    "            if not (\n",
    "                pos == 'PRON' or\n",
    "                (pos in ['NOUN', 'PROPN'] and tok.dep_ in ['nsubj', 'nsubjpass'])\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Shuffle words ===== #\n",
    "def shuffle_words(sentence, inside_percentage=100):\n",
    "    words = sentence.split()\n",
    "    n = int(len(words) * inside_percentage / 100)\n",
    "\n",
    "    if n < 2:\n",
    "        return sentence\n",
    "\n",
    "    indices = random.sample(range(len(words)), n)\n",
    "    shuffled = [words[i] for i in indices]\n",
    "    random.shuffle(shuffled)\n",
    "\n",
    "    for idx, new_word in zip(indices, shuffled):\n",
    "        words[idx] = new_word\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ===== Punctuation Errors ===== #\n",
    "def corrupt_punctuation(sentence, num_errors=2):\n",
    "    puncts = ['.', ',', '?', ';', ':', '!', '-']\n",
    "    words = sentence.split()\n",
    "    \n",
    "    if len(words) < 3:\n",
    "        return sentence  \n",
    "\n",
    "    indices = list(range(len(words)))\n",
    "    random.shuffle(indices)\n",
    "    indices = [i for i in indices if i != len(words) - 1] \n",
    "\n",
    "\n",
    "    for idx in indices[:num_errors]:\n",
    "        word = words[idx]\n",
    "        if random.random() < 0.5 and word[-1] in puncts:\n",
    "            words[idx] = word[:-1] + random.choice(puncts)\n",
    "        else:\n",
    "            words[idx] += random.choice(puncts)\n",
    "\n",
    "    if words[-1][-1] in puncts:\n",
    "        words[-1] = words[-1][:-1] + random.choice(puncts)\n",
    "    else:\n",
    "        words[-1] += random.choice(puncts)\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9aa6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ===== Articles and Prepositions Errors ===== #\n",
    "def corrupt_prepositions_and_articles(sentence, num_errors=3):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [t.text for t in doc]\n",
    "    new_tokens = tokens.copy()\n",
    "\n",
    "    # Liste base\n",
    "    preps = [\"in\", \"on\", \"at\", \"by\", \"with\", \"to\", \"from\", \"about\", \"under\", \"over\", \"for\"]\n",
    "    articles = [\"a\", \"an\", \"the\"]\n",
    "    demonstratives = [\"this\", \"that\", \"these\", \"those\"]\n",
    "    det_total = articles + demonstratives\n",
    "\n",
    "    targets = []\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.pos_ == \"ADP\" or token.lower_ in preps:\n",
    "            targets.append((i, \"prep\"))\n",
    "        elif token.pos_ == \"DET\" and token.lower_ in det_total:\n",
    "            if token.lower_ in articles:\n",
    "                targets.append((i, \"art\"))\n",
    "            elif token.lower_ in demonstratives:\n",
    "                targets.append((i, \"demo\"))\n",
    "\n",
    "    if not targets:\n",
    "        return sentence\n",
    "\n",
    "    sampled = random.sample(targets, min(num_errors, len(targets)))\n",
    "\n",
    "    for idx, tag_type in sampled:\n",
    "        action = random.choice([\"replace\", \"delete\", \"insert\"])\n",
    "        \n",
    "        if action == \"replace\":\n",
    "            if tag_type == \"prep\":\n",
    "                new_tokens[idx] = random.choice(preps)\n",
    "            elif tag_type == \"art\":\n",
    "                new_tokens[idx] = random.choice(articles)\n",
    "            elif tag_type == \"demo\":\n",
    "                new_tokens[idx] = random.choice(demonstratives)\n",
    "        \n",
    "        elif action == \"delete\":\n",
    "            new_tokens[idx] = \"\"\n",
    "        \n",
    "        elif action == \"insert\":\n",
    "            if tag_type == \"prep\":\n",
    "                word = random.choice(preps)\n",
    "            elif tag_type == \"art\":\n",
    "                word = random.choice(articles)\n",
    "            else:\n",
    "                word = random.choice(demonstratives)\n",
    "            insert_pos = random.randint(0, len(new_tokens)-1)\n",
    "            new_tokens.insert(insert_pos, word)\n",
    "\n",
    "    return \" \".join([w for w in new_tokens if w.strip()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply to sentence percentage\n",
    "def apply_to_percentage_of_sentences(sentences, func, sentence_percentage=50, **kwargs):\n",
    "    n = int(len(sentences) * sentence_percentage / 100)\n",
    "    indices = random.sample(range(len(sentences)), n)\n",
    "    new_sentences = deepcopy(sentences)\n",
    "    for i in indices:\n",
    "        new_sentences[i] = func(sentences[i], **kwargs)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417c0a2",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd51fc3",
   "metadata": {},
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulations_synt = {\n",
    "    \"verb_tense_errors\": alter_verb_tense,\n",
    "    \"shuffle_words\": shuffle_words,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import spacy\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def recompose_text(sentences):\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "def run_pipeline_synt(text, question, answer, anw_sentences, output_csv_path, manipulation_functions, runs_per_condition=4):\n",
    "    sentences = get_sentences(text)\n",
    "    results = []     \n",
    "\n",
    "    if output_csv_path is not None:\n",
    "        file = open(output_csv_path, mode=\"w\", newline='', encoding='utf-8')\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            \"timestamp\", \"manipulation\", \"percentage\", \"run\", \"WER\", \"CER\",\n",
    "            \"modified_text\", \"openai_answer\"\n",
    "        ])\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    for manipulation_name, func in manipulation_functions.items():\n",
    "        for sentence_percentage in [50, 100]:\n",
    "            for run in range(1, runs_per_condition + 1):\n",
    "                modified_sentences = apply_to_percentage_of_sentences(\n",
    "                    sentences, func, sentence_percentage=sentence_percentage\n",
    "                )\n",
    "\n",
    "                modified_text = recompose_text(modified_sentences)\n",
    "\n",
    "                openai_answer = generate_answer_rag_openai(question, modified_text)\n",
    "\n",
    "                wer = wer_metric.compute(predictions=[modified_text], references=[text])\n",
    "                cer = cer_metric.compute(predictions=[modified_text], references=[text])\n",
    "\n",
    "                result_dict = {\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"manipulation\": manipulation_name,\n",
    "                    \"percentage\": sentence_percentage,\n",
    "                    \"run\": run,\n",
    "                    \"WER\": round(wer, 4),\n",
    "                    \"CER\": round(cer, 4),\n",
    "                    \"modified_text\": modified_text,\n",
    "                    \"openai_answer\": openai_answer.strip()\n",
    "                }\n",
    "\n",
    "                if writer is not None:\n",
    "                    writer.writerow(list(result_dict.values()))\n",
    "                else:\n",
    "                    results.append(result_dict)\n",
    "\n",
    "    if writer is not None:\n",
    "        file.close()\n",
    "\n",
    "    return results if writer is None else {\"status\": \"saved_to_file\", \"path\": output_csv_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac566d23",
   "metadata": {},
   "source": [
    "FACTUALITY/INFDENSITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7294c9a",
   "metadata": {},
   "source": [
    "Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import wikipedia\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bert_score import score as bert_score\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# === MODELS ===\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def get_wikipedia_summary(entity, sentences=2):\n",
    "    try:\n",
    "        return wikipedia.summary(entity, sentences=sentences)\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "# ===== Dilute ===== #\n",
    "\n",
    "def dilute_text(text, max_sentences_per_entity=2, max_entities=3):\n",
    "    doc = nlp(text)\n",
    "    enriched_sentences = []\n",
    "    entities_added = 0\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if entities_added >= max_entities:\n",
    "            break\n",
    "        summary = get_wikipedia_summary(ent.text, sentences=max_sentences_per_entity)\n",
    "        if summary:\n",
    "            enriched_sentences.append(summary)\n",
    "            entities_added += 1\n",
    "\n",
    "    doc_sents = [sent.text.strip() for sent in doc.sents]\n",
    "    if enriched_sentences:\n",
    "        doc_sents.insert(0, enriched_sentences[0])\n",
    "        for enriched in enriched_sentences[1:]:\n",
    "            doc_sents.insert(random.randint(1, len(doc_sents)), enriched)\n",
    "\n",
    "    return \" \".join(doc_sents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17659b3b",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e276c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24622db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941523b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information Relevance\n",
    "def information_relevance(question, text):\n",
    "    embeddings = sbert_model.encode([question, text])\n",
    "    return util.cos_sim(embeddings[0], embeddings[1]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ab02f",
   "metadata": {},
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b85216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def run_pipeline_factInfDens(original_passage, answer, anw_sentences, output_path, question):\n",
    "    results = []  \n",
    "\n",
    "    # Se output_path è valido, apro il file\n",
    "    if output_path is not None:\n",
    "        file = open(output_path, mode=\"w\", newline='', encoding='utf-8')\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            \"timestamp\", \"manipulation\", \"setting\", \"run\", \"metric\", \"score\", \"modified_text\", \"openai_answer\"\n",
    "        ])\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    for manipulation, settings in [\n",
    "        (\"dilute\", [(2, 3), (4, 3)])\n",
    "    ]:\n",
    "        for setting in settings:\n",
    "            for run in range(4):\n",
    "                if  manipulation == \"dilute\":\n",
    "                    mod_text = dilute_text(original_passage, max_sentences_per_entity=setting[0], max_entities=setting[1])\n",
    "                    metric_score = information_relevance(question, mod_text)\n",
    "                    metric_name = \"information_relevance\"\n",
    "                    setting_str = f\"{setting[0]}s_{setting[1]}e\"\n",
    "\n",
    "\n",
    "                openai_answer = generate_answer_rag_openai(question, context=mod_text)\n",
    "\n",
    "                result_dict = {\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"manipulation\": manipulation,\n",
    "                    \"setting\": setting_str,\n",
    "                    \"run\": run + 1,\n",
    "                    \"metric\": metric_name,\n",
    "                    \"score\": round(metric_score, 4),\n",
    "                    \"modified_text\": mod_text,\n",
    "                    \"openai_answer\": openai_answer.strip()\n",
    "                }\n",
    "\n",
    "                if writer is not None:\n",
    "                    writer.writerow(list(result_dict.values()))\n",
    "                else:\n",
    "                    results.append(result_dict)\n",
    "\n",
    "    if writer is not None:\n",
    "        file.close()\n",
    "\n",
    "    return results if writer is None else {\"status\": \"saved_to_file\", \"path\": output_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49429aa8",
   "metadata": {},
   "source": [
    "RUN ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c036c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ftfy import fix_text\n",
    "\n",
    "def load_data_from_jsonl(file_path):\n",
    "    data_list = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # Estrai la domanda\n",
    "            question = data.get(\"input\", \"\")\n",
    "\n",
    "            # Estrai il primo passage (se presente)\n",
    "            passages = data.get(\"passages\", [])\n",
    "            if passages:\n",
    "                passage = passages[0]\n",
    "                raw_text = passage.get(\"text\", \"\")\n",
    "                text = fix_text(raw_text)\n",
    "                sentences = passage.get(\"sentences\", [])\n",
    "            else:\n",
    "                text, sentences = \"\", [], \"\"\n",
    "\n",
    "            # Estrai la risposta e le frasi selezionate\n",
    "            output = data.get(\"output\", [{}])[0]\n",
    "            answer = output.get(\"answer\", \"\")\n",
    "            selected_sentences = output.get(\"selected_sentences\", [])\n",
    "\n",
    "            # Aggiungi al dataset processato\n",
    "            data_list.append({\n",
    "                \"question\": question,\n",
    "                \"text\": text,\n",
    "                \"sentences\": sentences,\n",
    "                \"answer\": answer,\n",
    "                \"selected_sentences\": selected_sentences,\n",
    "            })\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195350ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"clapnqans.jsonl\"\n",
    "data = load_data_from_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4795c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "num_iterations = 1954\n",
    "\n",
    "df = pd.read_csv(\"words_to_modify.csv\")\n",
    "\n",
    "# === Grammar ===\n",
    "results_gr = []\n",
    "for i in range(num_iterations):\n",
    "    example = data[i]\n",
    "    text = example[\"text\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    anw_sentences = example[\"selected_sentences\"]\n",
    "\n",
    "    words_to_modify = df[\"words\"].iloc[i]\n",
    "    try:\n",
    "        words_to_modify = ast.literal_eval(words_to_modify)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    test_id = i\n",
    "    result = run_pipeline_gr(text, question, words_to_modify, answer, anw_sentences, None, manipulations_gr)\n",
    "\n",
    "    for r in result:\n",
    "        r[\"test_id\"] = test_id\n",
    "\n",
    "    results_gr.extend(result)\n",
    "    print(f\"COUNT GR: {i}\")\n",
    "\n",
    "pd.DataFrame(results_gr).to_csv(\n",
    "    \"results_gr.csv\",\n",
    "    index=False, encoding=\"utf-8\"\n",
    ")\n",
    "print(\"GR completed\")\n",
    "\n",
    "# === Syntax ===\n",
    "results_synt = []\n",
    "for i in range(num_iterations):\n",
    "    example = data[i]\n",
    "    text = example[\"text\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    anw_sentences = example[\"selected_sentences\"]\n",
    "\n",
    "    test_id = i \n",
    "    result = run_pipeline_synt(text, question, answer, anw_sentences, None, manipulations_synt)\n",
    "\n",
    "    for r in result:\n",
    "        r[\"test_id\"] = test_id\n",
    "    results_synt.extend(result)\n",
    "    print(f\"COUNT SYNT: {i}\")\n",
    "\n",
    "pd.DataFrame(results_synt).to_csv(\n",
    "    \"results_synt.csv\",\n",
    "    index=False, encoding=\"utf-8\"\n",
    ")\n",
    "print(\"SYNT completed\")\n",
    "\n",
    "# === Readability ===\n",
    "results_read = []\n",
    "for i in range(num_iterations):\n",
    "    example = data[i]\n",
    "    text = example[\"text\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    anw_sentences = example[\"selected_sentences\"]\n",
    "\n",
    "    words_to_modify = df[\"parole\"].iloc[i]\n",
    "    try:\n",
    "        words_to_modify = ast.literal_eval(words_to_modify)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    test_id = i\n",
    "    result = run_pipeline_read(text, question, words_to_modify, answer, anw_sentences, None, manipulations_read)\n",
    "    \n",
    "    for r in result:\n",
    "        r[\"test_id\"] = test_id\n",
    "    results_read.extend(result)\n",
    "    print(f\"COUNT READ: {i}\")\n",
    "\n",
    "pd.DataFrame(results_read).to_csv(\n",
    "    \"results_read.csv\",\n",
    "    index=False, encoding=\"utf-8\"\n",
    ")\n",
    "print(\"READ completed\")\n",
    "\n",
    "# === FactInfDens ===\n",
    "results_factInfDens = []\n",
    "for i in range(num_iterations):\n",
    "    example = data[i]\n",
    "    text = example[\"text\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    anw_sentences = example[\"selected_sentences\"]\n",
    "\n",
    "    test_id = i\n",
    "    result = run_pipeline_factInfDens(text, answer, anw_sentences, None, question)\n",
    "\n",
    "    for r in result:\n",
    "        r[\"test_id\"] = test_id\n",
    "    results_factInfDens.extend(result)\n",
    "    print(f\"COUNT FACINFDENS: {i}\")\n",
    "\n",
    "pd.DataFrame(results_factInfDens).to_csv(\n",
    "    \"results_factInfDens.csv\",\n",
    "    index=False, encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"FACINFDENS completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
