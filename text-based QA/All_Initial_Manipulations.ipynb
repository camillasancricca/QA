{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8963ac7d",
   "metadata": {},
   "source": [
    "LIBRERIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e747689",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bad060",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17456e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd251633",
   "metadata": {},
   "source": [
    "GRAMATICALLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d756f0",
   "metadata": {},
   "source": [
    "Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2277108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "boundary_punctuation = '.,!?;:()[]{}\"\\'“”‘’--'\n",
    "\n",
    "# ===== swap ===== #\n",
    "def swap_letters(word):\n",
    "\n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    \n",
    "    # Extract prefix punctuation\n",
    "    while len(word) > 0 and word[0] in boundary_punctuation:\n",
    "        prefix += word[0]\n",
    "        word = word[1:]\n",
    "    \n",
    "    # Extract suffix punctuation\n",
    "    while len(word) > 0 and word[-1] in boundary_punctuation:\n",
    "        suffix = word[-1] + suffix\n",
    "        word = word[:-1]\n",
    "    \n",
    "    # If too short, return unchanged\n",
    "    if len(word) < 4:\n",
    "        return prefix + word + suffix\n",
    "    \n",
    "    # Swap middle letters\n",
    "    middle = list(word[1:-1])\n",
    "    if len(middle) < 2:\n",
    "        return prefix + word + suffix\n",
    "\n",
    "    idx = random.randint(0, len(middle) - 2)\n",
    "    middle[idx], middle[idx + 1] = middle[idx + 1], middle[idx]\n",
    "    \n",
    "    swapped = word[0] + ''.join(middle) + word[-1]\n",
    "    return prefix + swapped + suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "boundary_punctuation = \".,;:!?()[]{}\\\"'\"\n",
    "\n",
    "# QWERTY keyboard neighbors map\n",
    "keyboard_neighbors = {\n",
    "    'a': 'qwsz', 'b': 'vghn', 'c': 'xdfv', 'd': 'serfcx',\n",
    "    'e': 'wsdr', 'f': 'drtgcv', 'g': 'ftyhbv', 'h': 'gyujnb',\n",
    "    'i': 'ujko', 'j': 'huikmn', 'k': 'jiolm', 'l': 'kop',\n",
    "    'm': 'njk', 'n': 'bhjm', 'o': 'iklp', 'p': 'ol',\n",
    "    'q': 'wa', 'r': 'edft', 's': 'awedxz', 't': 'rfgy',\n",
    "    'u': 'yhji', 'v': 'cfgb', 'w': 'qase', 'x': 'zsdc',\n",
    "    'y': 'tghu', 'z': 'asx',\n",
    "    '0': '9', '1': '2q', '2': '13w', '3': '24e', '4': '35r',\n",
    "    '5': '46t', '6': '57y', '7': '68u', '8': '79i', '9': '80o',\n",
    "    '+': '=-*ì',\n",
    "    '=': '-+',\n",
    "    '-': '=0_'\n",
    "}\n",
    "\n",
    "# ===== keyboard typo ===== #\n",
    "def keyboard_typo(word):\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    \n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    \n",
    "    while len(word) > 0 and word[0] in boundary_punctuation:\n",
    "        prefix += word[0]\n",
    "        word = word[1:]\n",
    "\n",
    "    while len(word) > 0 and word[-1] in boundary_punctuation:\n",
    "        suffix = word[-1] + suffix\n",
    "        word = word[:-1]\n",
    "\n",
    "    if len(word) == 0:\n",
    "        return prefix + suffix\n",
    "\n",
    "    # Generate the typo\n",
    "    idx = random.randint(0, len(word) - 1)\n",
    "    char = word[idx].lower()\n",
    "\n",
    "    if char in keyboard_neighbors:\n",
    "        replacement = random.choice(keyboard_neighbors[char])\n",
    "        word = word[:idx] + replacement + word[idx + 1:]\n",
    "\n",
    "    return prefix + word + suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ===== full random ===== #\n",
    "def fully_random(word):\n",
    "\n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    \n",
    "    while len(word) > 0 and word[0] in boundary_punctuation:\n",
    "        prefix += word[0]\n",
    "        word = word[1:]\n",
    "    \n",
    "    while len(word) > 0 and word[-1] in boundary_punctuation:\n",
    "        suffix = word[-1] + suffix\n",
    "        word = word[:-1]\n",
    "\n",
    "    letters = list(word)\n",
    "    random.shuffle(letters)\n",
    "    return prefix + ''.join(letters) + suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ===== middle random ===== #\n",
    "def middle_random(word):\n",
    "\n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    \n",
    "    while len(word) > 0 and word[0] in boundary_punctuation:\n",
    "        prefix += word[0]\n",
    "        word = word[1:]\n",
    "    \n",
    "    while len(word) > 0 and word[-1] in boundary_punctuation:\n",
    "        suffix = word[-1] + suffix\n",
    "        word = word[:-1]\n",
    "    \n",
    "    if len(word) < 4:\n",
    "        return prefix + word + suffix\n",
    "    \n",
    "    middle = list(word[1:-1])\n",
    "    random.shuffle(middle)\n",
    "    return prefix + word[0] + ''.join(middle) + word[-1] + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== key_random ===== #\n",
    "def typo_and_shuffle(word):\n",
    "    word_with_typo = keyboard_typo(word)\n",
    "    shuffled_word = fully_random(word_with_typo)\n",
    "    return shuffled_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c915533",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Remove ===== #\n",
    "def remove_letters(word, max_remove=3):\n",
    "\n",
    "    # Decide how many letters to remove (between 1 and 'max_remove')\n",
    "    num_to_remove = random.randint(1, max_remove)\n",
    "\n",
    "    word_list = list(word)\n",
    "    \n",
    "    for _ in range(num_to_remove):\n",
    "        if word_list: \n",
    "            index_to_remove = random.randint(0, len(word_list) - 1)\n",
    "            word_list.pop(index_to_remove)\n",
    "\n",
    "    return ''.join(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "boundary_punctuation = '.,!?;:\"\\'()[]{}'\n",
    "\n",
    "# ===== Random typo ===== #\n",
    "def random_typo(word):\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    \n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "\n",
    "    while len(word) > 0 and word[0] in boundary_punctuation:\n",
    "        prefix += word[0]\n",
    "        word = word[1:]\n",
    "\n",
    "    while len(word) > 0 and word[-1] in boundary_punctuation:\n",
    "        suffix = word[-1] + suffix\n",
    "        word = word[:-1]\n",
    "    \n",
    "    if len(word) == 0:\n",
    "        return prefix + suffix\n",
    "\n",
    "    # Number of letters to replace (between 1 and 3)\n",
    "    num_replacements = random.randint(1, min(3, len(word)))\n",
    "\n",
    "    # Select unique positions in the word\n",
    "    positions = random.sample(range(len(word)), num_replacements)\n",
    "    word_chars = list(word)\n",
    "\n",
    "    for idx in positions:\n",
    "        random_char = random.choice(string.ascii_letters + string.digits)\n",
    "        word_chars[idx] = random_char\n",
    "\n",
    "    return prefix + ''.join(word_chars) + suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arianna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 1. Download the dataset\n",
    "path = kagglehub.dataset_download(\"fazilbtopal/misspelled-words\")\n",
    "\n",
    "# 2. Load the correct CSV file\n",
    "df = pd.read_csv(f\"{path}/misspelled.csv\")\n",
    "\n",
    "# 3. Build a dictionary: correct word → list of common misspellings\n",
    "# Using 'label' as the correct word and 'input' as the misspelled word\n",
    "error_dict = df.groupby('label')['input'].apply(list).to_dict()\n",
    "\n",
    "# ===== Natural ===== #\n",
    "def natural_noise(word, error_dict):\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in error_dict:\n",
    "        return random.choice(error_dict[word_lower])\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3072b",
   "metadata": {},
   "source": [
    "READABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7e02a",
   "metadata": {},
   "source": [
    "Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ee23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#=========  Complex Synonyms  ======== #\n",
    "def sub_with_harder_synonyms(text, words):\n",
    "    for word in words:\n",
    "        synsets = wn.synsets(word)\n",
    "        synonyms = {lemma.name() for s in synsets for lemma in s.lemmas() if lemma.name().lower() != word.lower()}\n",
    "        longer_syns = [syn for syn in synonyms if len(syn) > len(word)]\n",
    "        if longer_syns:\n",
    "            harder_syn = sorted(longer_syns, key=len, reverse=True)[0].replace('_', ' ')\n",
    "            text = re.sub(r'\\b{}\\b'.format(re.escape(word)), harder_syn, text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Definition Insertion ======== #\n",
    "def add_definitions_to_words(text, words):\n",
    "    for word in words:\n",
    "        synsets = wn.synsets(word)\n",
    "        if synsets:\n",
    "            definition = synsets[0].definition()\n",
    "            text = re.sub(r'\\b{}\\b'.format(re.escape(word)), f\"{word} ({definition})\", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b813e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Definition Substitution ======== #\n",
    "def replace_with_definition(text, words):\n",
    "    for word in words:\n",
    "        synsets = wn.synsets(word)\n",
    "        if synsets:\n",
    "            definition = synsets[0].definition()\n",
    "            text = re.sub(r'\\b{}\\b'.format(re.escape(word)), f\"{definition}\", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Punctuation Removal ======== #\n",
    "def remove_or_modify_punctuation(text,words_to_change):\n",
    "    text = re.sub(r'[.,;!?]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef30381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Duplicate Words ======== #\n",
    "def duplicate_words(text, words_to_duplicate):\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    for w in words:\n",
    "        result.append(w)\n",
    "   \n",
    "        if w.lower() in [word.lower() for word in words_to_duplicate]:\n",
    "            result.append(w)\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e28aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "#========= Double Negation ======== #\n",
    "def double_negate_sentence_logically(sentence):\n",
    "    \"\"\"\n",
    "    Applica una doppia negazione a una frase, scegliendo in modo casuale tra più varianti.\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    if not sentence:\n",
    "        return sentence\n",
    "\n",
    "    if sentence[-1] not in '.!?':\n",
    "        sentence += '.'\n",
    "\n",
    "    sentence_core = sentence[0].lower() + sentence[1:-1]  \n",
    "\n",
    "    # Templates for double negation\n",
    "    templates = [\n",
    "        f\"It is not the case that {sentence_core} is not true.\",\n",
    "        f\"It cannot be said that {sentence_core} is not correct.\",\n",
    "        f\"There is no way that {sentence_core} is not valid.\",\n",
    "        f\"It would be false to claim that {sentence_core} is not true.\",\n",
    "        f\"It is not false that {sentence_core}.\",\n",
    "        f\"It is not untrue that {sentence_core}.\",\n",
    "    ]\n",
    "\n",
    "    return random.choice(templates)\n",
    "\n",
    "def double_negation_by_sentence(text, fraction, seed=None):\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    total = len(sentences)\n",
    "    n_to_change = round(fraction * total)\n",
    "\n",
    "    indices_to_change = set(random.sample(range(total), n_to_change))\n",
    "\n",
    "    transformed = [\n",
    "        double_negate_sentence_logically(s) if i in indices_to_change else s\n",
    "        for i, s in enumerate(sentences)\n",
    "    ]\n",
    "    return ' '.join(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0741247",
   "metadata": {},
   "source": [
    "SYNTATICALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyinflect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5233f1",
   "metadata": {},
   "source": [
    "Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77f896a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from copy import deepcopy\n",
    "\n",
    "# Carica modello spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Utility\n",
    "def get_sentences(text):\n",
    "    return [sent.text.strip() for sent in nlp(text).sents]\n",
    "\n",
    "def get_tokens_with_pos(sentence):\n",
    "    return [(token.text, token.pos_, token) for token in nlp(sentence)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c222f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyinflect\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_lemma(word):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "    return token._.inflect(\"VB\") or token.lemma_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Verb tense errors ===== #\n",
    "def alter_verb_tense(sentence, inside_percentage=100):\n",
    "    # Get tokens with part-of-speech tags\n",
    "    tokens = get_tokens_with_pos(sentence)\n",
    "    words = [w for w, _, _ in tokens]\n",
    "    \n",
    "    verb_indices = [i for i, (_, pos, _) in enumerate(tokens) if pos in [\"AUX\", \"VERB\"]]\n",
    "\n",
    "    n = int(len(verb_indices) * inside_percentage / 100)\n",
    "    indices_to_change = random.sample(verb_indices, n) if n < len(verb_indices) else verb_indices\n",
    "\n",
    "    for i in indices_to_change:\n",
    "        word, pos, token = tokens[i]\n",
    "        lower_word = word.lower()\n",
    "\n",
    "        # Concord errors: is/are, was/were\n",
    "        if lower_word == \"is\":\n",
    "            words[i] = \"are\"\n",
    "        elif lower_word == \"are\":\n",
    "            words[i] = \"is\"\n",
    "        elif lower_word == \"was\":\n",
    "            words[i] = \"were\"\n",
    "        elif lower_word == \"were\":\n",
    "            words[i] = \"was\"\n",
    "        else:\n",
    "            # Change verb tense (e.g., past -> base)\n",
    "            lemma = token.lemma_\n",
    "            if token.tag_ in [\"VBD\", \"VBN\"]:  # past\n",
    "                new_form = token._.inflect(\"VB\")  # base form\n",
    "            else:  # present or base\n",
    "                new_form = token._.inflect(\"VBD\")  # past form\n",
    "\n",
    "            if new_form:\n",
    "                words[i] = new_form\n",
    "            else:\n",
    "                words[i] = lemma \n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff15027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Remove Subject/Verb ===== #\n",
    "def remove_tokens_by_type(sentence, remove_type='verb'):\n",
    "    tokens = get_tokens_with_pos(sentence)\n",
    "\n",
    "    if remove_type == 'verb':\n",
    "\n",
    "        filtered_words = [\n",
    "            word for word, pos, _ in tokens\n",
    "            if pos not in ['VERB', 'AUX']\n",
    "        ]\n",
    "    elif remove_type == 'noun':\n",
    "\n",
    "        filtered_words = [\n",
    "            word for word, pos, tok in tokens\n",
    "            if not (\n",
    "                pos == 'PRON' or\n",
    "                (pos in ['NOUN', 'PROPN'] and tok.dep_ in ['nsubj', 'nsubjpass'])\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Shuffle words ===== #\n",
    "def shuffle_words(sentence, inside_percentage=100):\n",
    "    words = sentence.split()\n",
    "    n = int(len(words) * inside_percentage / 100)\n",
    "\n",
    "    if n < 2:\n",
    "        return sentence\n",
    "\n",
    "    indices = random.sample(range(len(words)), n)\n",
    "    shuffled = [words[i] for i in indices]\n",
    "    random.shuffle(shuffled)\n",
    "\n",
    "    for idx, new_word in zip(indices, shuffled):\n",
    "        words[idx] = new_word\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ===== Punctuation Errors ===== #\n",
    "def corrupt_punctuation(sentence, num_errors=2):\n",
    "    puncts = ['.', ',', '?', ';', ':', '!', '-']\n",
    "    words = sentence.split()\n",
    "    \n",
    "    if len(words) < 3:\n",
    "        return sentence  \n",
    "\n",
    "    indices = list(range(len(words)))\n",
    "    random.shuffle(indices)\n",
    "    indices = [i for i in indices if i != len(words) - 1] \n",
    "\n",
    "\n",
    "    for idx in indices[:num_errors]:\n",
    "        word = words[idx]\n",
    "        if random.random() < 0.5 and word[-1] in puncts:\n",
    "            words[idx] = word[:-1] + random.choice(puncts)\n",
    "        else:\n",
    "            words[idx] += random.choice(puncts)\n",
    "\n",
    "    if words[-1][-1] in puncts:\n",
    "        words[-1] = words[-1][:-1] + random.choice(puncts)\n",
    "    else:\n",
    "        words[-1] += random.choice(puncts)\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9aa6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ===== Articles and Prepositions Errors ===== #\n",
    "def corrupt_prepositions_and_articles(sentence, num_errors=3):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [t.text for t in doc]\n",
    "    new_tokens = tokens.copy()\n",
    "\n",
    "    # Liste base\n",
    "    preps = [\"in\", \"on\", \"at\", \"by\", \"with\", \"to\", \"from\", \"about\", \"under\", \"over\", \"for\"]\n",
    "    articles = [\"a\", \"an\", \"the\"]\n",
    "    demonstratives = [\"this\", \"that\", \"these\", \"those\"]\n",
    "    det_total = articles + demonstratives\n",
    "\n",
    "    targets = []\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.pos_ == \"ADP\" or token.lower_ in preps:\n",
    "            targets.append((i, \"prep\"))\n",
    "        elif token.pos_ == \"DET\" and token.lower_ in det_total:\n",
    "            if token.lower_ in articles:\n",
    "                targets.append((i, \"art\"))\n",
    "            elif token.lower_ in demonstratives:\n",
    "                targets.append((i, \"demo\"))\n",
    "\n",
    "    if not targets:\n",
    "        return sentence\n",
    "\n",
    "    sampled = random.sample(targets, min(num_errors, len(targets)))\n",
    "\n",
    "    for idx, tag_type in sampled:\n",
    "        action = random.choice([\"replace\", \"delete\", \"insert\"])\n",
    "        \n",
    "        if action == \"replace\":\n",
    "            if tag_type == \"prep\":\n",
    "                new_tokens[idx] = random.choice(preps)\n",
    "            elif tag_type == \"art\":\n",
    "                new_tokens[idx] = random.choice(articles)\n",
    "            elif tag_type == \"demo\":\n",
    "                new_tokens[idx] = random.choice(demonstratives)\n",
    "        \n",
    "        elif action == \"delete\":\n",
    "            new_tokens[idx] = \"\"\n",
    "        \n",
    "        elif action == \"insert\":\n",
    "            if tag_type == \"prep\":\n",
    "                word = random.choice(preps)\n",
    "            elif tag_type == \"art\":\n",
    "                word = random.choice(articles)\n",
    "            else:\n",
    "                word = random.choice(demonstratives)\n",
    "            insert_pos = random.randint(0, len(new_tokens)-1)\n",
    "            new_tokens.insert(insert_pos, word)\n",
    "\n",
    "    return \" \".join([w for w in new_tokens if w.strip()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac566d23",
   "metadata": {},
   "source": [
    "FACTUALITY/INFDENSITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7294c9a",
   "metadata": {},
   "source": [
    "Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import wikipedia\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bert_score import score as bert_score\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# === MODELS ===\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "nli_model_raw = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "nli_pipeline = pipeline(\"text-classification\", model=nli_model_raw, tokenizer=nli_tokenizer)\n",
    "\n",
    "# === FAKE & VAGUE ENTITIES ===\n",
    "fake_entities = {\n",
    "    \"GPE\": [\"Spain\", \"Canada\", \"Brazil\", \"Australia\", \"Italy\", \"Germany\", \"Mexico\", \"Norway\", \"Japan\", \"India\", \"Russia\", \"Egypt\", \"Morocco\", \"Chile\", \"Argentina\", \"Portugal\", \"Sweden\", \"South Africa\", \"New Zealand\", \"Belgium\"],\n",
    "    \"PERSON\": [\"John Doe\", \"Jane Smith\", \"Alex Johnson\", \"Chris Lee\", \"Patricia Brown\", \"Michael Davis\", \"Linda Garcia\", \"James Wilson\", \"Elizabeth Martinez\", \"David Anderson\", \"Jennifer Thomas\", \"Robert Jackson\", \"Mary White\", \"Charles Harris\", \"Barbara Martin\", \"Paul Thompson\", \"Nancy Moore\", \"Mark Lewis\", \"Susan Clark\", \"George Walker\"],\n",
    "    \"ORG\": [\"Acme Corp\", \"Globex Corporation\", \"Umbrella Inc.\", \"Soylent Corp\", \"Initech\", \"Stark Industries\", \"Wayne Enterprises\", \"Cyberdyne Systems\", \"Wonka Industries\", \"Oscorp\", \"Tyrell Corporation\", \"Aperture Science\", \"Vandelay Industries\", \"Gringotts Bank\", \"Dunder Mifflin\", \"Hooli\", \"Virtucon\", \"Monsters Inc.\", \"Gekko & Co.\", \"Pied Piper\"],\n",
    "    \"LOC\": [\"the mountains\", \"the desert\", \"the coast\", \"the valley\", \"the forest\", \"the plains\", \"the riverbank\", \"the hills\", \"the lake shore\", \"the wetlands\", \"the canyon\", \"the plateau\", \"the islands\", \"the tundra\", \"the savannah\", \"the cliffs\", \"the caves\", \"the bay\", \"the marshes\", \"the jungle\"]\n",
    "}\n",
    "\n",
    "vague_entities = {\n",
    "    \"GPE\": [\"a country\", \"a region\", \"a territory\", \"somewhere\", \"a place\"],\n",
    "    \"PERSON\": [\"a person\", \"someone\", \"an individual\", \"a figure\", \"an actor\"],\n",
    "    \"ORG\": [\"an organization\", \"a company\", \"a group\", \"an institution\", \"an entity\"],\n",
    "    \"LOC\": [\"a place\", \"somewhere\", \"a location\", \"an area\", \"a site\"]\n",
    "}\n",
    "\n",
    "# === TEXT MODIFIERS ===\n",
    "def replace_entities(text, mode=\"fake\"):\n",
    "    doc = nlp(text)\n",
    "    new_text = text\n",
    "    offset = 0\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in (fake_entities if mode == \"fake\" else vague_entities):\n",
    "            replacement = random.choice(fake_entities[ent.label_] if mode == \"fake\" else vague_entities[ent.label_])\n",
    "            start, end = ent.start_char + offset, ent.end_char + offset\n",
    "            new_text = new_text[:start] + replacement + new_text[end:]\n",
    "            offset += len(replacement) - (end - start)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "# ===== Fake information ===== #\n",
    "def insert_fake_information(text, percent=100):\n",
    "    doc = nlp(text)\n",
    "    ents = list(doc.ents)\n",
    "    num_to_change = max(1, int(len(ents) * percent / 100))\n",
    "    selected = random.sample(ents, num_to_change)\n",
    "    \n",
    "    for ent in selected:\n",
    "        if ent.label_ in fake_entities:\n",
    "            text = text.replace(ent.text, random.choice(fake_entities[ent.label_]))\n",
    "    \n",
    "    text = re.sub(r'\\bNo\\.?\\s*\\d+\\b', lambda m: f\"No. {random.randint(10, 99)}\", text)\n",
    "    text = re.sub(r'\\b(17[5-9][0-9]|18[0-9]{2})\\b', str(random.randint(1750, 1799)), text)\n",
    "    return text\n",
    "\n",
    "# ===== Simplify ===== #\n",
    "def simplify_text(text, percent=100):\n",
    "    doc = nlp(text)\n",
    "    ents = list(doc.ents)\n",
    "    num_to_change = max(1, int(len(ents) * percent / 100))\n",
    "    selected = random.sample(ents, num_to_change)\n",
    "\n",
    "    for ent in selected:\n",
    "        if ent.label_ in vague_entities:\n",
    "            text = text.replace(ent.text, random.choice(vague_entities[ent.label_]))\n",
    "\n",
    "    if percent == 100:\n",
    "        text = re.sub(r'\\b\\d{4}\\b', '', text)\n",
    "        text = re.sub(r'\\bNo\\.?\\s*\\d+\\b', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_wikipedia_summary(entity, sentences=2):\n",
    "    try:\n",
    "        return wikipedia.summary(entity, sentences=sentences)\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "# ===== Dilute ===== #\n",
    "\n",
    "def dilute_text(text, max_sentences_per_entity=2, max_entities=3):\n",
    "    doc = nlp(text)\n",
    "    enriched_sentences = []\n",
    "    entities_added = 0\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if entities_added >= max_entities:\n",
    "            break\n",
    "        summary = get_wikipedia_summary(ent.text, sentences=max_sentences_per_entity)\n",
    "        if summary:\n",
    "            enriched_sentences.append(summary)\n",
    "            entities_added += 1\n",
    "\n",
    "    doc_sents = [sent.text.strip() for sent in doc.sents]\n",
    "    if enriched_sentences:\n",
    "        doc_sents.insert(0, enriched_sentences[0])\n",
    "        for enriched in enriched_sentences[1:]:\n",
    "            doc_sents.insert(random.randint(1, len(doc_sents)), enriched)\n",
    "\n",
    "    return \" \".join(doc_sents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542522b8",
   "metadata": {},
   "source": [
    "COHERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e98857",
   "metadata": {},
   "source": [
    "Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39699910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "# ===== Insert Random Words ===== #\n",
    "def insert_random_words_by_keyword_count(text: str, selected_words: list[str], manipulation_name: str) -> str:\n",
    "    words = text.split()\n",
    "    num_to_insert = len(selected_words)\n",
    "\n",
    "    for _ in range(num_to_insert):\n",
    "        fake_word = faker.word() \n",
    "        insert_pos = random.randint(0, len(words)) \n",
    "        words.insert(insert_pos, fake_word)\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "# ===== Insert Random Sentences ===== #\n",
    "def insert_random_sentences_by_keyword_count(text: str, selected_words: list[str], manipulation_name: str) -> str:\n",
    "    sentences = text.split(\". \")  \n",
    "    num_to_insert = len(selected_words)\n",
    "\n",
    "    for _ in range(num_to_insert):\n",
    "        fake_sentence = faker.sentence(nb_words=random.randint(6, 12))\n",
    "        insert_pos = random.randint(0, len(sentences)) \n",
    "        sentences.insert(insert_pos, fake_sentence.strip())\n",
    "\n",
    "    return \". \".join(sentences).strip() + ('.' if not text.strip().endswith('.') else '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import re\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "# ===== Replace Keywords ===== #\n",
    "def replace_keywords_in_text(text: str, selected_words: list[str], manipulation_name: str) -> str:\n",
    "    def generate_fake_word():\n",
    "        return faker.word()\n",
    "\n",
    "    replacement_map = {}\n",
    "    for keyword in selected_words:\n",
    "        fake_word = generate_fake_word()\n",
    "\n",
    "        if keyword.istitle():\n",
    "            fake_word = fake_word.title()\n",
    "        elif keyword.isupper():\n",
    "            fake_word = fake_word.upper()\n",
    "\n",
    "        replacement_map[keyword] = fake_word\n",
    "\n",
    "    def replace_match(match):\n",
    "        word = match.group(0)\n",
    "        return replacement_map.get(word, word)\n",
    "\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(word) for word in selected_words) + r')\\b'\n",
    "    modified_text = re.sub(pattern, replace_match, text)\n",
    "\n",
    "    return modified_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20745613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ===== Add Negatives ===== #\n",
    "def add_negatives(text: str, selected_words: list[str], manipulation_name: str) -> str:\n",
    "    # Funzione di sostituzione che aggiunge 'not' prima della parola\n",
    "    def insert_not(match):\n",
    "        word = match.group(0)\n",
    "        return f\"not {word}\"\n",
    "    \n",
    "    pattern = r'\\b(' + '|'.join(re.escape(word) for word in selected_words) + r')\\b'\n",
    "    modified_text = re.sub(pattern, insert_not, text)\n",
    "\n",
    "    return modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6329bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "def get_antonym(word: str) -> str:\n",
    "    \"\"\"Restituisce il primo antonimo trovato di una parola, oppure None.\"\"\"\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                return lemma.antonyms()[0].name()\n",
    "    return None\n",
    "\n",
    "# ===== Replays Keywors with Antonyms ===== #\n",
    "def replace_keywords_antonym(text: str, selected_words: list[str], manipulation_name: str = \"\") -> str:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    named_entity_positions = set()\n",
    "    for ent in doc.ents:\n",
    "        for i in range(ent.start_char, ent.end_char):\n",
    "            named_entity_positions.add(i)\n",
    "\n",
    "    def replace_match(match):\n",
    "        word = match.group(0)\n",
    "        span_range = range(match.start(), match.end())\n",
    "\n",
    "        if any(i in named_entity_positions for i in span_range):\n",
    "            return word\n",
    "\n",
    "        lower_word = word.lower()\n",
    "        antonym = get_antonym(lower_word)\n",
    "        if antonym:\n",
    "            return antonym.capitalize() if word[0].isupper() else antonym\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    for word in selected_words:\n",
    "        word_regex = r'\\b' + re.escape(word) + r'\\b'\n",
    "        text = re.sub(word_regex, replace_match, text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9b7e7",
   "metadata": {},
   "source": [
    "SENTENCE FLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4363a",
   "metadata": {},
   "source": [
    "Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#====== Shuffle Sentences =====#\n",
    "def shuffle_sentences_by_percentage(sentences, percentage):\n",
    "\n",
    "    n = len(sentences)\n",
    "    if n == 0:\n",
    "        return sentences[:]\n",
    "\n",
    "    k = max(1, int(n * percentage))  \n",
    "\n",
    "    indices = random.sample(range(n), k)\n",
    "    selected = [sentences[i] for i in indices]\n",
    "\n",
    "    while True:\n",
    "        shuffled = selected[:]\n",
    "        random.shuffle(shuffled)\n",
    "        if shuffled != selected:\n",
    "            break\n",
    "\n",
    "    new_sentences = sentences[:]\n",
    "    for idx, new_sentence in zip(indices, shuffled):\n",
    "        new_sentences[idx] = new_sentence\n",
    "\n",
    "    return new_sentences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
